# =============================================================================
# Atelier Prefect - EXERCICES (Approche Fil Rouge)
# =============================================================================
#
# üéØ OBJECTIF : Transformer un script ML classique en pipeline de production
#
# Vous allez progressivement "blinder" un vrai pipeline de pr√©diction de Churn
# avec tous les patterns d'orchestration professionnels :
#
#   √âTAPE 1 : Tasks & Flows (transformer les fonctions en t√¢ches orchestr√©es)
#   √âTAPE 2 : R√©silience (ajouter des r√©essais sur le chargement)
#   √âTAPE 3 : Efficacit√© (cacher le preprocessing co√ªteux + parall√©lisme)
#   √âTAPE 4 : MLflow + sklearn Pipeline (tracker avec la meilleure pratique)
#   √âTAPE 5 : Orchestration (organiser en sous-flows)
#   √âTAPE 6 : D√©ploiement (planifier l'ex√©cution automatique)
#   √âTAPE 7 : Notifications (alertes Discord/Slack en cas d'√©chec)
#
# PR√âREQUIS (√† faire UNE SEULE FOIS avant de commencer) :
#
#   1. V√©rifiez que Docker est lanc√© :
#      docker-compose ps
#
#   2. Copiez le fichier .env.example et renommez-le en .env
#      (ou en terminal : cp .env.example .env sur Mac/Linux, copy .env.example .env sur Windows)
#
#   C'est tout ! Le fichier .env configure automatiquement la connexion
#   aux serveurs Prefect et MLflow dans Docker.
#
# EX√âCUTION :
#   python Prefect_Exercises.py etape1    # Commencer ici !
#   python Prefect_Exercises.py etape2    # Puis continuer...
#   python Prefect_Exercises.py etape3
#   python Prefect_Exercises.py etape4
#   python Prefect_Exercises.py etape5
#   python Prefect_Exercises.py deploy    # Le clou du spectacle !
#   python Prefect_Exercises.py notif     # Bonus : alertes
#
# INTERFACES :
#   Prefect : http://localhost:4200
#   MLflow  : http://localhost:5000
#
# R√âF√âRENCE : Consultez Prefect_Workshop.py pour voir les solutions compl√®tes.
#
# =============================================================================

import sys
import os
import time
from pathlib import Path
from datetime import timedelta, datetime

import mlflow
import pandas as pd
import numpy as np

# Charger automatiquement le fichier .env (PREFECT_API_URL, MLFLOW_TRACKING_URI)
from dotenv import load_dotenv
PROJECT_ROOT = Path(__file__).resolve().parents[3]
load_dotenv(PROJECT_ROOT / ".env")

# Configuration
DATA_PATH = PROJECT_ROOT / "data" / "customer_data.csv"
PREDICTIONS_PATH = PROJECT_ROOT / "data" / "predictions_exercises.csv"
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")

# =============================================================================
# LE PIPELINE DE CHURN - Code de d√©part (sans orchestration)
# =============================================================================
#
# Voici un pipeline ML classique. Votre mission : le transformer en pipeline
# de production robuste avec Prefect !
#
# =============================================================================


def load_churn_data(file_path: str = DATA_PATH) -> pd.DataFrame:
    """
    Charger les donn√©es clients.

    Dans un contexte r√©el, cela pourrait √™tre une requ√™te vers une base de
    donn√©es ou une API qui peut √©chouer temporairement.
    """
    print(f"üìÇ Chargement des donn√©es depuis {file_path}...")

    if Path(file_path).exists():
        df = pd.read_csv(file_path)
    else:
        # Donn√©es synth√©tiques si le fichier n'existe pas
        print("‚ö†Ô∏è  Fichier non trouv√©, g√©n√©ration de donn√©es synth√©tiques...")
        np.random.seed(42)
        n_samples = 500
        df = pd.DataFrame({
            'customer_id': range(1, n_samples + 1),
            'age': np.random.randint(18, 70, n_samples),
            'recency_days': np.random.randint(1, 365, n_samples),
            'frequency': np.random.randint(1, 50, n_samples),
            'monetary_value': np.random.uniform(10, 1000, n_samples),
            'churned': np.random.binomial(1, 0.3, n_samples)
        })

    print(f"‚úÖ Charg√© {len(df)} clients")
    return df


def preprocess_data(df: pd.DataFrame) -> tuple:
    """
    Pr√©traitement des donn√©es.

    Cette √©tape est souvent co√ªteuse (feature engineering, scaling, encoding).
    C'est un candidat id√©al pour le cache !
    """
    print("üîß Preprocessing des donn√©es...")
    time.sleep(1)  # Simule un preprocessing co√ªteux

    # Feature engineering
    feature_cols = ['recency_days', 'frequency', 'monetary_value', 'age']
    feature_cols = [c for c in feature_cols if c in df.columns]

    if not feature_cols:
        feature_cols = [c for c in df.columns if c not in ['customer_id', 'churned']]

    X = df[feature_cols].fillna(0)
    y = df['churned'] if 'churned' in df.columns else pd.Series(np.zeros(len(df)))

    # Normalisation simple
    X = (X - X.mean()) / (X.std() + 1e-8)

    print(f"‚úÖ Features : {list(X.columns)}, Shape : {X.shape}")
    return X, y


def train_model(X: pd.DataFrame, y: pd.Series, n_estimators: int = 100, max_depth: int = 10) -> dict:
    """
    Entra√Æner un mod√®le RandomForest.

    Les hyperparam√®tres devraient √™tre configurables !
    """
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

    print(f"üéØ Entra√Ænement du mod√®le (n_estimators={n_estimators}, max_depth={max_depth})...")

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=42
    )
    model.fit(X_train, y_train)

    # √âvaluation
    y_pred = model.predict(X_test)
    metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred)
    }

    print(f"‚úÖ Accuracy : {metrics['accuracy']:.4f}, F1 : {metrics['f1']:.4f}")
    return {"model": model, "metrics": metrics, "feature_cols": list(X.columns)}


def save_predictions(df: pd.DataFrame, model, feature_cols: list) -> str:
    """Sauvegarder les pr√©dictions."""
    print("üíæ Sauvegarde des pr√©dictions...")

    X_pred = df[feature_cols].fillna(0)
    X_pred = (X_pred - X_pred.mean()) / (X_pred.std() + 1e-8)

    predictions = model.predict(X_pred)
    df_out = df[['customer_id']].copy()
    df_out['predicted_churn'] = predictions

    df_out.to_csv(PREDICTIONS_PATH, index=False)
    print(f"‚úÖ Pr√©dictions sauvegard√©es : {PREDICTIONS_PATH}")
    return PREDICTIONS_PATH


# =============================================================================
# √âTAPE 1 : TASKS & FLOWS - Les Bases
# =============================================================================
#
# OBJECTIF : Transformer les fonctions ci-dessus en t√¢ches Prefect
#
# CONCEPTS :
#   - @task : Transforme une fonction en t√¢che orchestr√©e (logs, UI, etc.)
#   - @flow : Orchestre plusieurs t√¢ches, point d'entr√©e du pipeline
#   - Les donn√©es circulent via les valeurs de retour (pas de fichiers !)
#
# =============================================================================

def run_etape1():
    """
    √âTAPE 1 : Cr√©er votre premier flow Prefect

    Transformez le code m√©tier en t√¢ches orchestr√©es :
    1. D√©corer load_churn_data comme une @task
    2. D√©corer preprocess_data comme une @task
    3. D√©corer train_model comme une @task
    4. Cr√©er un @flow qui les orchestre
    """
    from prefect import flow, task

    print("=" * 70)
    print("√âTAPE 1 : TASKS & FLOWS - Transformer le code en pipeline Prefect")
    print("=" * 70)

    # -------------------------------------------------------------------------
    # TODO : Ajoutez le d√©corateur @task √† cette fonction
    # INDICE : @task se place juste au-dessus de "def"
    # -------------------------------------------------------------------------
    @task(name="chargement-donnees")
    def load_data() -> pd.DataFrame:
        """Charger les donn√©es clients."""
        return load_churn_data(DATA_PATH)

    # -------------------------------------------------------------------------
    # TODO : Ajoutez le d√©corateur @task √† cette fonction
    # -------------------------------------------------------------------------
    @task(name="Preparation-features")
    def prepare_features(df: pd.DataFrame) -> tuple:
        """Pr√©parer les features."""
        return preprocess_data(df)

    # -------------------------------------------------------------------------
    # TODO : Ajoutez le d√©corateur @task √† cette fonction
    # -------------------------------------------------------------------------
    @task(name="entrainement-modele")
    def train(X: pd.DataFrame, y: pd.Series) -> dict:
        """Entra√Æner le mod√®le."""
        return train_model(X, y)
    
    @task(name="sauvegarde-predictions")
    def save_pred(df: pd.DataFrame, model, feature_cols: list) -> str:
        """Sauvegarder les pr√©dictions."""
        return save_predictions(df, model, feature_cols)

    # -------------------------------------------------------------------------
    # TODO : Cr√©ez un flow qui orchestre ces t√¢ches
    # INDICE : @flow(name="churn-pipeline-v1", log_prints=True)
    # -------------------------------------------------------------------------
    @flow(name="churn-pipeline-v1", log_prints=True)
    def churn_pipeline_v1():
        """
        Pipeline de pr√©diction de Churn - Version 1

        TODO : Appelez les t√¢ches dans l'ordre :
        1. data = load_data()
        2. X, y = prepare_features(data)
        3. result = train(X, y)
        4. Retournez result
        """
        # TODO : Compl√©tez le pipeline
        data = load_data()       # <-- Remplacez par load_data()
        X, y = prepare_features(data)  # <-- Remplacez par prepare_features(data)
        result = train(X, y)     # <-- Remplacez par train(X, y)
        save_pred(data, result['model'], result['feature_cols'])
        if result:
            print(f"üéâ Pipeline termin√© ! Accuracy : {result['metrics']['accuracy']:.4f}")

        return result

    # Ex√©cuter
    print("\nüìã Instructions :")
    print("   1. Ajoutez @task aux 3 fonctions (load_data, prepare_features, train)")
    print("   2. Ajoutez @flow √† churn_pipeline_v1")
    print("   3. Compl√©tez les appels dans le flow\n")

    try:
        result = churn_pipeline_v1()
        if result and result.get('metrics', {}).get('accuracy', 0) > 0:
            print("\n‚úÖ √âTAPE 1 R√âUSSIE !")
            print("   Vous avez cr√©√© votre premier pipeline Prefect !")
            print("   Passez √† l'√©tape 2 : python Prefect_Exercises.py etape2")
        else:
            print("\n‚ùå Le flow n'a pas retourn√© de r√©sultat.")
            print("   V√©rifiez que vous avez remplac√© les None par les appels de fonctions.")
    except Exception as e:
        print(f"\n‚ùå Erreur : {e}")
        print("   INDICE : Avez-vous ajout√© les d√©corateurs @task et @flow ?")
        
    # Rappel : Pour √©x√©cuter cette √©tape, utilisez la commande :
    # uv run .\pipelines\workshop\02_prefect\Prefect_Exercises.py etape1 (depuis le terminal √† la racine du projet)

    # -------------------------------------------------------------------------
    # MINI-D√âFIS (apr√®s avoir compl√©t√© l'exercice) :
    # -------------------------------------------------------------------------
    #
    # D√âFI 1.1 : Ouvrez http://localhost:4200 et retrouvez votre flow.
    #            Explorez les d√©tails de l'ex√©cution (t√¢ches, dur√©es, logs).
    #
    # D√âFI 1.2 : Ajoutez name="chargement-donnees" au d√©corateur @task de
    #            load_data. Regardez comment √ßa change l'affichage dans l'UI.
    #
    # D√âFI 1.3 : Ajoutez une 4√®me t√¢che 'save_results' qui sauvegarde les
    #            m√©triques dans un fichier JSON.
    # -------------------------------------------------------------------------


# =============================================================================
# √âTAPE 2 : R√âSILIENCE - R√©essais Automatiques
# =============================================================================
#
# OBJECTIF : Rendre le chargement de donn√©es robuste aux √©checs transitoires
#
# CONCEPTS :
#   - retries : Nombre de tentatives apr√®s √©chec
#   - retry_delay_seconds : D√©lai entre les tentatives
#   - Backoff exponentiel : [5, 15, 30] = attendre de plus en plus longtemps
#
# SC√âNARIO R√âEL : Une base de donn√©es temporairement indisponible, une API
# avec rate limiting, un fichier r√©seau inaccessible...
#
# =============================================================================

def run_etape2():
    """
    √âTAPE 2 : Ajouter la r√©silience au chargement de donn√©es

    Le chargement de donn√©es peut √©chouer (r√©seau, base de donn√©es...).
    Configurez des r√©essais automatiques !
    """
    from prefect import flow, task
    import random

    print("=" * 70)
    print("√âTAPE 2 : R√âSILIENCE - R√©essais automatiques sur le chargement")
    print("=" * 70)

    # -------------------------------------------------------------------------
    # TODO : Configurez cette t√¢che pour r√©essayer 3 fois avec backoff [5, 10, 20]
    # INDICE : @task(retries=3, retry_delay_seconds=[5, 10, 20])
    # -------------------------------------------------------------------------
    @task(retries=3, retry_delay_seconds=[2, 8, 32])
    def load_data_with_retry() -> pd.DataFrame:
        """
        Charger les donn√©es avec simulation d'√©checs.

        Dans un vrai sc√©nario, remplacez random.random() par une vraie
        requ√™te vers une base de donn√©es ou une API.
        """
        # Simulation d'√©chec transitoire (40% de chance d'√©chec)
        if random.random() < 0.8:
            print("‚ùå Connexion √©chou√©e ! (simulation)")
            raise ConnectionError("Database temporarily unavailable")

        return load_churn_data(DATA_PATH)

    @task
    def prepare_features(df: pd.DataFrame) -> tuple:
        return preprocess_data(df)

    @task
    def train(X: pd.DataFrame, y: pd.Series) -> dict:
        return train_model(X, y)

    @flow(name="churn-pipeline-v2-resilient", log_prints=True)
    def churn_pipeline_v2():
        """Pipeline avec r√©silience."""
        data = load_data_with_retry()
        X, y = prepare_features(data)
        result = train(X, y)
        return result

    # Ex√©cuter
    print("\nüìã Instructions :")
    print("   1. Ajoutez retries=3 et retry_delay_seconds=[5, 10, 20] √† load_data_with_retry")
    print("   2. Ex√©cutez plusieurs fois pour voir les r√©essais en action\n")

    random.seed(None)  # Seed al√©atoire pour voir les √©checs

    try:
        result = churn_pipeline_v2()
        print("\n‚úÖ √âTAPE 2 R√âUSSIE !")
        print("   Votre pipeline est maintenant r√©silient aux √©checs transitoires !")
        print("   Passez √† l'√©tape 3 : python Prefect_Exercises.py etape3")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Le pipeline a √©chou√© apr√®s tous les r√©essais : {e}")
        print("   C'est normal si la malchance s'acharne ! R√©ex√©cutez.")
        print("   V√©rifiez que vous avez bien configur√© retries=3.")

    # -------------------------------------------------------------------------
    # MINI-D√âFIS :
    # -------------------------------------------------------------------------
    #
    # D√âFI 2.1 : Changez la probabilit√© d'√©chec de 0.4 √† 0.8.
    #            Combien de fois voyez-vous "Connexion √©chou√©e !" avant succ√®s ?
    #
    # D√âFI 2.2 : Ajoutez un backoff plus agressif : [2, 8, 32] secondes.
    #            Chronom√©trez le temps total en cas d'√©checs multiples.
    #            (Globalement, vous devriez voir des temps d'attente de plus en plus longs : 2s, puis 8s, puis 32s
    #            (et avec peu de retry c'est possible que le Flux s'arr√™te)
    #
    # D√âFI 2.3 : Dans un vrai projet, o√π mettriez-vous les r√©essais ?
    #            (Indice : pas sur le preprocessing !)
    # -------------------------------------------------------------------------
    
    # Rappel : Pour √©x√©cuter cette √©tape, utilisez la commande :
    # uv run .\pipelines\workshop\02_prefect\Prefect_Exercises.py etape2 (depuis le terminal √† la racine du projet)


# =============================================================================
# √âTAPE 3 : EFFICACIT√â - Cache du Preprocessing + Parall√©lisme
# =============================================================================
#
# OBJECTIF : √âviter de refaire le preprocessing si les donn√©es n'ont pas chang√©,
#            et entra√Æner plusieurs mod√®les simultan√©ment
#
# CONCEPTS CACHE :
#   - cache_key_fn : Fonction qui g√©n√®re une cl√© de cache (hash des inputs)
#   - cache_expiration : Dur√©e de validit√© du cache
#   - task_input_hash : Fonction pr√™te √† l'emploi pour hasher les inputs
#
# CONCEPTS PARALL√âLISME :
#   - task()        : Ex√©cution s√©quentielle (attend la fin avant de continuer)
#   - task.submit() : Ex√©cution parall√®le (retourne un Future imm√©diatement)
#   - future.result() : R√©cup√®re le r√©sultat quand la t√¢che est termin√©e
#
# SC√âNARIO R√âEL : Feature engineering co√ªteux (30 min), donn√©es qui ne changent
# que quotidiennement ‚Üí cacher pendant 1h √©vite 90% des recalculs !
#
# =============================================================================

def run_etape3():
    """
    √âTAPE 3 : Cacher le preprocessing co√ªteux

    Le preprocessing prend 1 seconde (simul√©). Sur de vraies donn√©es,
    √ßa peut prendre 30 minutes ! Configurez le cache.
    """
    from prefect import flow, task
    from prefect.tasks import task_input_hash

    print("=" * 70)
    print("√âTAPE 3 : EFFICACIT√â - Cache du preprocessing")
    print("=" * 70)

    @task(retries=3, retry_delay_seconds=[5, 10, 20])
    def load_data() -> pd.DataFrame:
        return load_churn_data(DATA_PATH)

    # -------------------------------------------------------------------------
    # TODO : Activez le cache sur cette t√¢che
    # INDICE : @task(cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))
    # -------------------------------------------------------------------------
    @task(cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))
    def prepare_features_cached(df: pd.DataFrame) -> tuple:
        """
        Preprocessing avec cache.

        La premi√®re ex√©cution prendra ~1 seconde.
        Les suivantes seront instantan√©es (cache hit) !

        NOTE: Lors d'un cache hit, Prefect n'ex√©cute PAS le code de la fonction.
        Le print ci-dessous n'appara√Ætra donc pas la 2√®me fois !
        """
        print("‚è≥ Preprocessing en cours (1 seconde)...")
        return preprocess_data(df)

    @task
    def train(X: pd.DataFrame, y: pd.Series, n_estimators: int = 100) -> dict:
        return train_model(X, y, n_estimators=n_estimators)

    @flow(name="churn-pipeline-v3-cached", log_prints=True)
    def churn_pipeline_v3():
        """Pipeline avec cache."""
        data = load_data()

        print("\n=== Premi√®re ex√©cution du preprocessing ===")
        start = time.time()
        X, y = prepare_features_cached(data)
        time1 = time.time() - start
        print(f"‚è±Ô∏è  Temps : {time1:.2f}s")

        print("\n=== Deuxi√®me ex√©cution (devrait utiliser le cache) ===")
        start = time.time()
        X2, y2 = prepare_features_cached(data)  # M√™mes donn√©es = cache hit !
        time2 = time.time() - start
        print(f"‚è±Ô∏è  Temps : {time2:.2f}s")

        # V√©rification du cache
        if time2 < 0.5 and time1 > 0.8:
            print("\n‚úÖ CACHE FONCTIONNE ! La 2√®me ex√©cution √©tait instantan√©e.")
            print("   (Notez que le print 'Preprocessing en cours' n'est pas apparu !)")
        else:
            print("\n‚ö†Ô∏è  Le cache ne semble pas actif. V√©rifiez les param√®tres.")

        result = train(X, y)
        return result

    # Ex√©cuter
    print("\nüìã Instructions :")
    print("   1. Ajoutez cache_key_fn=task_input_hash √† prepare_features_cached")
    print("   2. Ajoutez cache_expiration=timedelta(hours=1)")
    print("   3. Observez les temps d'ex√©cution\n")

    try:
        result = churn_pipeline_v3()
        print("\n‚úÖ √âTAPE 3 TERMIN√âE !")
        print("   Passez √† l'√©tape 4 : python Prefect_Exercises.py etape4")
    except Exception as e:
        print(f"\n‚ùå Erreur : {e}")

    # -------------------------------------------------------------------------
    # MINI-D√âFIS :
    # -------------------------------------------------------------------------
    #
    # D√âFI 3.1 : Changez cache_expiration √† timedelta(seconds=10).
    #            Attendez 15 secondes et r√©ex√©cutez. Le cache est-il invalid√© ?
    #
    # D√âFI 3.2 : Modifiez l√©g√®rement le DataFrame (ajoutez une colonne).
    #            Que se passe-t-il avec le cache ?
    #
    # D√âFI 3.3 : O√π NE PAS mettre de cache ? (Indice : l'entra√Ænement du mod√®le
    #            avec des hyperparam√®tres diff√©rents ne devrait PAS √™tre cach√©)
    #
    # D√âFI 3.4 (BONUS) : PARALL√âLISME - Entra√Æner deux mod√®les simultan√©ment
    # -------------------------------------------------------------------------
    # Dans Prefect, appeler une t√¢che normalement = s√©quentiel.
    # Utiliser .submit() = parall√®le : les deux t√¢ches d√©marrent en m√™me temps !
    # -------------------------------------------------------------------------
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.metrics import accuracy_score
    from sklearn.model_selection import train_test_split

    @task
    def train_rf(X_train, y_train, X_test, y_test) -> dict:
        model = RandomForestClassifier(n_estimators=50, random_state=42)
        model.fit(X_train, y_train)
        return {"name": "RandomForest", "accuracy": accuracy_score(y_test, model.predict(X_test))}

    @task
    def train_gb(X_train, y_train, X_test, y_test) -> dict:
        model = GradientBoostingClassifier(n_estimators=50, random_state=42)
        model.fit(X_train, y_train)
        return {"name": "GradientBoosting", "accuracy": accuracy_score(y_test, model.predict(X_test))}

    @flow(name="parallel-training-demo", log_prints=True)
    def parallel_training_demo():
        data = load_churn_data(DATA_PATH)
        X, y = preprocess_data(data)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # VERSION PARALL√àLE avec .submit() :
        rf_future = train_rf.submit(X_train, y_train, X_test, y_test)  # d√©marre imm√©diatement
        gb_future = train_gb.submit(X_train, y_train, X_test, y_test)  # d√©marre aussi !
        rf, gb = rf_future.result(), gb_future.result()                 # attend les deux

        best = max([rf, gb], key=lambda x: x["accuracy"])
        print(f"Meilleur mod√®le : {best['name']} ({best['accuracy']:.4f})")

    print("\n" + "=" * 60)
    print("D√âFI 3.4 : PARALL√âLISME")
    print("=" * 60)
    print("S√âQUENTIEL ‚Üí rf d√©marre, finit, PUIS gb d√©marre")
    print("PARALL√àLE  ‚Üí rf ET gb d√©marrent en m√™me temps (.submit())")
    print("Observez dans l'UI Prefect : les deux t√¢ches s'ex√©cutent simultan√©ment !\n")
    parallel_training_demo()
    # -------------------------------------------------------------------------
    # Rappel : Pour √©xecuter cette √©tape :


# =============================================================================
# √âTAPE 4 : MLFLOW - Tracking des Exp√©rimentations
# =============================================================================
#
# OBJECTIF : Logger les param√®tres, m√©triques et mod√®les dans MLflow
#
# CONCEPTS :
#   - Prefect g√®re : r√©essais, cache, planification, logs d'ex√©cution
#   - MLflow g√®re : param√®tres, m√©triques, artefacts, versioning des mod√®les
#   - Les appels MLflow se font DANS les t√¢ches Prefect
#   - sklearn Pipeline : combine scaler + mod√®le en UN SEUL artefact (best practice !)
#
# =============================================================================

def run_etape4():
    """
    √âTAPE 4 : Int√©grer MLflow pour le tracking

    Ajoutez le tracking MLflow avec sklearn Pipeline.
    """
    from prefect import flow, task
    from prefect.tasks import task_input_hash
    from sklearn.pipeline import Pipeline as SklearnPipeline
    from sklearn.preprocessing import StandardScaler

    # V√©rifier si MLflow est disponible
    try:
        import mlflow
        import mlflow.sklearn
        MLFLOW_AVAILABLE = True
    except ImportError:
        MLFLOW_AVAILABLE = False
        print("‚ö†Ô∏è  MLflow non install√©. Exercice en mode simulation.")

    print("=" * 70)
    print("√âTAPE 4 : MLFLOW + SKLEARN PIPELINE")
    print("=" * 70)
    print("""
POURQUOI SKLEARN PIPELINE ?

Au lieu de g√©rer scaler et mod√®le s√©par√©ment :
   scaler.fit_transform(X)
   model.fit(X_scaled, y)
   mlflow.log_artifact("scaler.pkl")  # Artefact 1
   mlflow.log_model(model)            # Artefact 2
   # ‚Üí 2 artefacts, risque d'oublier le scaler √† l'inf√©rence !

On combine tout dans un Pipeline :
   pipeline = Pipeline([('scaler', StandardScaler()), ('model', RF())])
   pipeline.fit(X, y)
   mlflow.log_model(pipeline)  # UN SEUL artefact !
   # ‚Üí √Ä l'inf√©rence : pipeline.predict(X) fait TOUT
""")

    @task(retries=3, retry_delay_seconds=[5, 10, 20])
    def load_data() -> pd.DataFrame:
        return load_churn_data(DATA_PATH)

    @task(cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))
    def prepare_features(df: pd.DataFrame) -> tuple:
        return preprocess_data(df)

    # -------------------------------------------------------------------------
    # TODO : Compl√©tez l'int√©gration MLflow avec sklearn Pipeline
    # -------------------------------------------------------------------------
    @task
    def train_with_mlflow(
        X: pd.DataFrame,
        y: pd.Series,
        n_estimators: int,
        max_depth: int,
        experiment_name: str
    ) -> dict:
        """Entra√Æner avec sklearn Pipeline et logger dans MLflow."""
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score, f1_score

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # ---------------------------------------------------------------------
        # TODO : Cr√©ez le sklearn Pipeline
        # INDICE :
        #   pipeline = SklearnPipeline([
        #       ('scaler', StandardScaler()),
        #       ('model', RandomForestClassifier(n_estimators=..., max_depth=..., random_state=42))
        #   ])
        # ---------------------------------------------------------------------
        pipeline = SklearnPipeline([
            ('scaler', StandardScaler()),
            ('model', RandomForestClassifier(
                n_estimators=n_estimators,
                max_depth=max_depth,
                random_state=42
            ))
        ])

        # Entra√Æner le pipeline (scaler + mod√®le en une seule ligne)
        pipeline.fit(X_train, y_train)

        # Pr√©dire (le pipeline applique automatiquement le scaling)
        y_pred = pipeline.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        if not MLFLOW_AVAILABLE:
            print(f"üìä [SIMULATION] Pipeline entra√Æn√© : Accuracy={accuracy:.4f}")
            return {"pipeline": pipeline, "accuracy": accuracy, "run_id": "simulation"}

        # ---------------------------------------------------------------------
        # TODO : Configurez MLflow
        # INDICE : mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
        #          mlflow.set_experiment(experiment_name='prefect-training')
        # ---------------------------------------------------------------------
        pass  # <-- Remplacez par la configuration MLflow
        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
        mlflow.set_experiment(experiment_name='prefect-training')

        # ---------------------------------------------------------------------
        # TODO : D√©marrez un run MLflow et loggez le pipeline
        # INDICE : with mlflow.start_run(run_name="prefect-training"):
        # ---------------------------------------------------------------------
        # D√©commentez et compl√©tez :
        
        with mlflow.start_run(run_name=f"training-{datetime.now().strftime('%H%M%S')}"):
        
            # TODO : Loggez les param√®tres
            mlflow.log_params({"n_estimators": n_estimators, "max_depth": max_depth})
        
            # TODO : Loggez les m√©triques
            mlflow.log_metrics({"accuracy": accuracy, "f1": f1})
        
            # TODO : Loggez le PIPELINE (scaler + mod√®le en UN artefact !)
            mlflow.sklearn.log_model(pipeline, name="model")
            # NOTE : Par d√©faut, MLflow s√©rialise en pickle (risque de s√©curit√©).
            # Alternatives possibles selon le contexte :
            #
            # - skops (recommand√© en production, Python uniquement) :
            #   mlflow.sklearn.log_model(pipeline, name="model", serialization_format="skops")
            #   ‚úÖ Plus s√ªr (pas d'ex√©cution de code arbitraire √† la d√©s√©rialisation)
            #   ‚úÖ Support√© nativement par MLflow
            #   ‚ùå Plus r√©cent ‚Üí ecosyst√®me moins mature, moins de ressources en ligne
            #   ‚ùå Moins flexible (certains objets custom non support√©s)
            #   ‚ùå Python uniquement, comme pickle
            #
            # - ONNX (interop√©rable, multi-langages) :
            #   ‚úÖ Permet de servir le mod√®le depuis Java, C#, JavaScript, etc.
            #   ‚úÖ Id√©al si l'inf√©rence se fait hors Python (API Java, appli mobile...)
            #   ‚úÖ Performances d'inf√©rence souvent meilleures (runtime optimis√©)
            #   ‚ùå Conversion parfois complexe, tous les mod√®les ne sont pas support√©s
            #   ‚ùå Le Pipeline sklearn peut poser des probl√®mes √† la conversion
            #   import onnxmltools; mlflow.onnx.log_model(...)
        
            # ---------------------------------------------------------------------
            # TODO : Loggez la matrice de confusion avec mlflow.log_figure()
            #
            # La figure est d√©j√† cr√©√©e ci-dessous, il vous reste √† la logger !
            # INDICE : mlflow.log_figure(fig, "confusion_matrix.png")
            # ---------------------------------------------------------------------
            from sklearn.metrics import ConfusionMatrixDisplay
            import matplotlib.pyplot as plt

            fig, ax = plt.subplots()
            ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)
            ax.set_title("Matrice de confusion")

            # TODO : Loggez la figure dans MLflow ici
            mlflow.log_figure(fig, "confusion_matrix.png")

            plt.close(fig)  # Lib√©rer la m√©moire

            run_id = mlflow.active_run().info.run_id
            print(f"‚úÖ MLflow Run ID : {run_id}")
            print(f"   Accuracy : {accuracy:.4f}, F1 : {f1:.4f}")
            print(f"   ‚≠ê Pipeline logg√© (scaler + model en UN artefact)")
        
        return {"pipeline": pipeline, "accuracy": accuracy, "f1": f1, "run_id": run_id}

        # En attendant que vous compl√©tiez, version sans tracking :
        print(f"‚ö†Ô∏è  MLflow non configur√©. Accuracy : {accuracy:.4f}")
        return {"pipeline": pipeline, "accuracy": accuracy, "run_id": None}

    # -------------------------------------------------------------------------
    # TODO : Ajoutez des param√®tres au flow avec des valeurs par d√©faut
    # INDICE : def churn_pipeline_v4(n_estimators: int = 100, max_depth: int = 10):
    # -------------------------------------------------------------------------
    @flow(name="churn-pipeline-v4-mlflow", log_prints=True)
    def churn_pipeline_v4(n_estimators: int = 100, max_depth: int = 10):  # <-- Ajoutez les param√®tres ici
        """Pipeline avec int√©gration MLflow."""
        data = load_data()
        X, y = prepare_features(data)
        
        # TODO : D√©fi 4.3 : Enregistrez le mod√®le dans le Model Registry avec mlflow.register_model()
        # INDICE : mlflow.register_model(f"runs:/{run_id}/model", "churn-model")
        mlflow.register_model(f"runs:/{train_with_mlflow(X, y, n_estimators, max_depth, 'prefect-churn-exercises')['run_id']}/model", "churn-model")

        # TODO : Utilisez les param√®tres du flow au lieu de valeurs fixes
        result = train_with_mlflow(
            X, y,
            n_estimators=n_estimators,  # <-- Remplacez par le param√®tre du flow
            max_depth=max_depth,      # <-- Remplacez par le param√®tre du flow
            experiment_name="prefect-churn-exercises"
        )

        return result

    # Ex√©cuter
    print("\nüìã Instructions :")
    print("   1. D√©commentez le bloc MLflow dans train_with_mlflow")
    print("   2. Ajoutez des param√®tres au flow (n_estimators, max_depth)")
    print("   3. V√©rifiez les r√©sultats dans http://localhost:5000\n")

    if MLFLOW_AVAILABLE:
        print(f"üìä MLflow tracking : {MLFLOW_TRACKING_URI}\n")

    try:
        # result = churn_pipeline_v4(n_estimators=100, max_depth=10)
        for n_est, depth in [(50, 5), (200, 20), (150, 15)]:
            result = churn_pipeline_v4(n_estimators=n_est, max_depth=depth)
        print("\n‚úÖ √âTAPE 4 TERMIN√âE !")
        print("   Passez √† l'√©tape 5 : python Prefect_Exercises.py etape5")
        if MLFLOW_AVAILABLE and result.get("run_id"):
            print(f"   Voir le run : {MLFLOW_TRACKING_URI}")
    except Exception as e:
        print(f"\n‚ùå Erreur : {e}")

    # -------------------------------------------------------------------------
    # MINI-D√âFIS :
    # -------------------------------------------------------------------------
    #
    # D√âFI 4.1 : Ex√©cutez le flow 3 fois avec diff√©rents hyperparam√®tres.
    #            Comparez les r√©sultats dans l'interface MLflow.
    #
    # D√âFI 4.2 : La matrice de confusion est d√©j√† g√©n√©r√©e dans train_with_mlflow.
    #            Loggez-la dans MLflow avec mlflow.log_figure() et v√©rifiez qu'elle appara√Æt dans l'UI.
    #            Elle devrait se trouver dans l'onglet "Artifacts" de MLflow !
    #
    # D√âFI 4.3 : Utilisez mlflow.register_model() pour enregistrer le mod√®le
    #            dans le Model Registry avec le nom "churn-model".
    # -------------------------------------------------------------------------


# =============================================================================
# √âTAPE 5 : ORCHESTRATION - Organisation en Sous-Flows
# =============================================================================
#
# OBJECTIF : Structurer le pipeline en sous-flows r√©utilisables
#
# CONCEPTS :
#   - Sous-flows : Des flows appel√©s par d'autres flows
#   - Modularit√© : R√©utiliser le preprocessing dans diff√©rents pipelines
#   - Lisibilit√© : S√©parer les responsabilit√©s
#
# =============================================================================

def run_etape5():
    """
    √âTAPE 5 : Organiser en sous-flows

    Structurez le pipeline en modules r√©utilisables :
    - data_preparation_flow : Chargement + preprocessing
    - training_flow : Entra√Ænement + √©valuation
    - main_pipeline : Orchestre les sous-flows
    """
    from prefect import flow, task
    from prefect.tasks import task_input_hash

    print("=" * 70)
    print("√âTAPE 5 : ORCHESTRATION - Organisation en sous-flows")
    print("=" * 70)

    # === T√ÇCHES ===

    @task(retries=3, retry_delay_seconds=[5, 10, 20])
    def load_data() -> pd.DataFrame:
        return load_churn_data(DATA_PATH)

    @task(cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))
    def prepare_features(df: pd.DataFrame) -> tuple:
        return preprocess_data(df)

    @task
    def train(X, y, n_estimators: int, max_depth: int) -> dict:
        return train_model(X, y, n_estimators=n_estimators, max_depth=max_depth)

    @task
    def save_preds(df: pd.DataFrame, model, feature_cols: list) -> str:
        return save_predictions(df, model, feature_cols)

    # -------------------------------------------------------------------------
    # TODO : Cr√©ez un sous-flow pour la pr√©paration des donn√©es
    # INDICE : @flow(name="data-preparation")
    # -------------------------------------------------------------------------
    @flow(name="data-preparation", log_prints=True)
    def data_preparation_flow() -> tuple:
        """
        Sous-flow : Pr√©paration des donn√©es

        Charge les donn√©es et applique le preprocessing.
        Retourne (data, X, y) pour √™tre utilis√© par d'autres flows.
        """
        # TODO : Appeler load_data() puis prepare_features()
        data = load_data()
        X, y = prepare_features(data)
        return data, X, y

    # -------------------------------------------------------------------------
    # TODO : Cr√©ez un sous-flow pour l'entra√Ænement
    # INDICE : @flow(name="model-training")
    # -------------------------------------------------------------------------
    @flow(name="model-training", log_prints=True)
    def training_flow(X, y, n_estimators: int = 100, max_depth: int = 10, experiment_name: str = "prefect-churn-exercises") -> dict:
        """
        Sous-flow : Entra√Ænement du mod√®le

        Entra√Æne et √©value le mod√®le avec les hyperparam√®tres donn√©s.
        """
        # TODO : D√©corer cette fonction avec @flow
        # TODO : Appeler train()
        result = train(X, y, n_estimators=n_estimators, max_depth=max_depth)
        return result
    
    # -------------------------------------------------------------------------
    # D√©fi 5.1 : Flow inference_pipeline
    # TODO : Cr√©ez le flow 'inference_pipeline' ici qui r√©utilise data_preparation_flow
    #        mais charge un mod√®le existant (au lieu de l'entra√Æner).
    # -------------------------------------------------------------------------
    @flow(name="inference-pipeline", log_prints=True)
    def inference_pipeline():
        """
        Pipeline d'inf√©rence - R√©utilise la pr√©paration des donn√©es

        Ce flow pourrait √™tre utilis√© pour faire des pr√©dictions sur de nouvelles donn√©es
        en r√©utilisant data_preparation_flow, mais en chargeant un mod√®le pr√©-entra√Æn√©
        au lieu de l'entra√Æner √† nouveau.
        """
        # TODO : Appeler data_preparation_flow pour obtenir data, X, y
        data, X, y = data_preparation_flow()

        # TODO : Charger un mod√®le existant (par exemple depuis MLflow ou un fichier)
        #       et faire des pr√©dictions avec save_preds()
        #       (Vous pouvez simuler le chargement du mod√®le si vous n'avez pas encore de mod√®le enregistr√©)
        model = mlflow.sklearn.load_model("models:/churn-model/latest")  # Nom enregistr√© au D√©fi 4.3 via mlflow.register_model()
        feature_cols = X.columns.tolist() if X is not None else []
        
        if model:
            save_preds(data, model, feature_cols)
            print("‚úÖ Pr√©dictions sauvegard√©es avec le mod√®le charg√©.")
        else:
            print("‚ö†Ô∏è  Aucun mod√®le charg√©. Simulez le chargement pour tester save_preds.")

    # -------------------------------------------------------------------------
    # TODO : Cr√©ez le flow principal qui orchestre les sous-flows
    # -------------------------------------------------------------------------
    @flow(name="churn-pipeline-v5-modular", log_prints=True)
    def churn_pipeline_v5(n_estimators: int = 100, max_depth: int = 10, save_predictions: bool = True, experiment_name: str = "prefect-churn-exercises") -> dict:
        """
        Pipeline principal - Orchestre les sous-flows.

        Cette structure permet de :
        - R√©utiliser data_preparation_flow dans d'autres pipelines (inf√©rence)
        - Tester training_flow ind√©pendamment
        - Avoir une vue claire de l'architecture
        """
        print("üöÄ D√©marrage du pipeline modulaire...")

        # √âtape 1 : Pr√©paration des donn√©es (sous-flow)
        data, X, y = data_preparation_flow()

        if X is None:
            print("‚ùå La pr√©paration des donn√©es a √©chou√©.")
            print("   V√©rifiez que data_preparation_flow est bien un @flow")
            return None

        # √âtape 2 : Entra√Ænement (sous-flow)
        result = training_flow(X, y, n_estimators, max_depth, experiment_name=experiment_name)

        if result is None:
            print("‚ùå L'entra√Ænement a √©chou√©.")
            return None

        # √âtape 3 : Sauvegarde des pr√©dictions (optionnel)
        if save_predictions and result.get("model"):
            save_preds(data, result["model"], result["feature_cols"])

        print(f"üéâ Pipeline termin√© ! Accuracy : {result['metrics']['accuracy']:.4f}")

        # ---------------------------------------------------------------------
        # D√âFI 5.1 : Lancez le pipeline d'inf√©rence √† la suite de l'entra√Ænement !
        # D√©commentez la ligne ci-dessous une fois qu'un mod√®le est enregistr√©
        # dans MLflow (via mlflow.register_model() au D√©fi 4.3).
        #
        inference_pipeline()
        # ---------------------------------------------------------------------

        return result

    # Ex√©cuter
    print("\nüìã Instructions :")
    print("   1. Ajoutez @flow √† data_preparation_flow")
    print("   2. Ajoutez @flow √† training_flow")
    print("   3. Compl√©tez les appels dans chaque sous-flow\n")

    try:
        result = churn_pipeline_v5()
        if result:
            print("\n‚úÖ √âTAPE 5 R√âUSSIE !")
            print("   Votre pipeline est maintenant modulaire et r√©utilisable !")
            print("   Passez √† l'√©tape finale : python Prefect_Exercises.py deploy")


    except Exception as e:
        print(f"\n‚ùå Erreur : {e}")
        print("   V√©rifiez les d√©corateurs @flow sur les sous-flows.")

    # -------------------------------------------------------------------------
    # MINI-D√âFIS :
    # -------------------------------------------------------------------------
    #
    # D√âFI 5.1 : Cr√©ez un flow 'inference_pipeline' qui r√©utilise
    #            data_preparation_flow mais charge un mod√®le existant.
    #
    # D√âFI 5.2 : Dans l'interface Prefect, trouvez le graphe d'ex√©cution.
    #            Voyez-vous les sous-flows imbriqu√©s ? 
    #            (Il y a un bouton permettant de cacher/montrer les sous-flows disponible pour mieux les rep√©rer)
    #
    # D√âFI 5.3 : Ajoutez un param√®tre 'experiment_name' au flow principal
    #            et propagez-le jusqu'√† training_flow.
    #            (Notez qu'il est d√©j√† pr√©sent dans training_flow, mais il n'est pas encore utilis√© dans le flow principal)
    # -------------------------------------------------------------------------


# =============================================================================
# √âTAPE 6 : D√âPLOIEMENT - Automatisation avec Planification
# =============================================================================
#
# üéØ C'EST LE CLOU DU SPECTACLE !
#
# OBJECTIF : D√©ployer le pipeline pour qu'il s'ex√©cute automatiquement
#
# CONCEPTS :
#   - flow.serve() : D√©marre un serveur qui √©coute les planifications
#   - cron : Expression de planification (comme les crontabs Linux)
#      - " * * * * * " : Chaque √©toile repr√©sente une unit√© de temps (minute, heure, jour du mois, mois, jour de la semaine)
#      - Exemples :
#         - "0 12 * * *" : Tous les jours √† 12h
#         - "*/2 * * * *" : Toutes les 2 minutes
#         - "0 0 * * 0" : Tous les dimanches √† minuit
#   - Prefect UI : Visualiser les ex√©cutions automatiques
#
# SC√âNARIO R√âEL : R√©entra√Æner le mod√®le tous les jours √† 6h du matin
#
# =============================================================================

def run_deploy():
    """
    √âTAPE 6 : D√©ployer le pipeline planifi√©

    Le pipeline va s'ex√©cuter automatiquement toutes les 2 minutes !
    Observez dans l'interface Prefect.
    """
    from prefect import flow, task
    from prefect.tasks import task_input_hash

    print("=" * 70)
    print("√âTAPE 6 : D√âPLOIEMENT - Le pipeline s'ex√©cute tout seul !")
    print("=" * 70)

    # Pipeline complet avec tous les patterns
    @task(retries=3, retry_delay_seconds=[5, 10, 20])
    def load_data() -> pd.DataFrame:
        return load_churn_data(DATA_PATH)

    @task(cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))
    def prepare_features(df: pd.DataFrame) -> tuple:
        return preprocess_data(df)

    @task
    def train(X, y, n_estimators: int, max_depth: int) -> dict:
        return train_model(X, y, n_estimators=n_estimators, max_depth=max_depth)

    @flow(name="churn-production-pipeline", log_prints=True)
    def churn_production_pipeline(n_estimators: int = 100, max_depth: int = 10):
        """
        Pipeline de production - Pr√™t pour le d√©ploiement !

        Ce pipeline sera ex√©cut√© automatiquement selon la planification.
        """
        print(f"üöÄ Ex√©cution planifi√©e √† {datetime.now().strftime('%H:%M:%S')}")

        data = load_data()
        X, y = prepare_features(data)
        result = train(X, y, n_estimators, max_depth)

        print(f"‚úÖ Termin√© ! Accuracy : {result['metrics']['accuracy']:.4f}")
        return result

    print("\nüìã Instructions :")
    print("   1. Le pipeline va √™tre d√©ploy√© avec une planification cron")
    print("   2. Il s'ex√©cutera toutes les 2 minutes automatiquement")
    print("   3. Ouvrez http://localhost:4200 pour observer les ex√©cutions")
    print("   4. Appuyez sur Ctrl+C pour arr√™ter\n")

    print("=" * 70)
    print("üéØ D√âPLOIEMENT EN COURS...")
    print("=" * 70)

    # -------------------------------------------------------------------------
    # TODO : D√©ployez le flow avec une planification
    # INDICE : churn_production_pipeline.serve(
    #              name="churn-scheduled",
    #              cron="*/2 * * * *"  # Toutes les 2 minutes
    #          )
    # -------------------------------------------------------------------------
    
    churn_production_pipeline.serve(
        name="churn-scheduled",
        cron="*/2 * * * *",  # Toutes les 2 minutes
        tags=["production", "churn", "exercises"] # Tags 
    )

    # En attendant que vous d√©commentiez, ex√©cution simple :
    print("\n‚ö†Ô∏è  Pour activer le d√©ploiement planifi√©, d√©commentez le bloc")
    print("   churn_production_pipeline.serve(...) dans le code.\n")
    print("   Ex√©cution unique en attendant...\n")

    churn_production_pipeline()

    print("\n" + "=" * 70)
    print("‚úÖ Pour voir la VRAIE automatisation :")
    print("   1. D√©commentez le bloc .serve() dans le code")
    print("   2. R√©ex√©cutez : python Prefect_Exercises.py deploy")
    print("   3. Observez les runs appara√Ætre dans http://localhost:4200")
    print("=" * 70)
    # Note : Le serveur Prefect continuera de tourner et d'ex√©cuter le pipeline selon la planification.
    # Appuyez sur Ctrl+C pour l'arr√™ter quand vous avez fini de tester.
    # Votre terminal est utilis√© pour le serveur Prefect, donc vous ne verrez pas les prints du pipeline tant que le serveur tourne. C'est normal !

    # -------------------------------------------------------------------------
    # MINI-D√âFIS :
    # -------------------------------------------------------------------------
    #
    # D√âFI 6.1 : Changez la planification pour "0 6 * * *" (tous les jours √† 6h).
    #            C'est un pattern commun pour le r√©entra√Ænement quotidien !
    #
    # D√âFI 6.2 : Ajoutez des tags pour organiser vos d√©ploiements
    #            (ex: "production", "ml", "churn").
    #
    # D√âFI 6.3 : Cr√©ez un 2√®me d√©ploiement pour l'inf√©rence avec une planification
    #            diff√©rente (ex: toutes les heures pour des pr√©dictions batch).
    # -------------------------------------------------------------------------


# =============================================================================
# √âTAPE 7 : NOTIFICATIONS - Alertes en cas d'√©chec
# =============================================================================
#
# OBJECTIF : Recevoir une notification Discord/Slack quand le pipeline √©choue
#
# CONCEPTS :
#   - on_failure : Liste de handlers appel√©s en cas d'√©chec
#   - Webhooks : URLs pour envoyer des messages √† Discord/Slack
#   - Monitoring proactif : √ätre alert√© avant que les utilisateurs ne le soient
#
# =============================================================================

def run_notifications():
    """
    √âTAPE 7 (BONUS) : Notifications Discord/Slack

    Configurez des alertes automatiques en cas d'√©chec du pipeline.
    """
    from prefect import flow, task
    import json

    print("=" * 70)
    print("√âTAPE 7 : NOTIFICATIONS - Alertes Discord/Slack")
    print("=" * 70)

    # -------------------------------------------------------------------------
    # Configuration des webhooks (√† remplacer par vos URLs)
    # -------------------------------------------------------------------------
    DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL", "")
    SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL", "")

    # -------------------------------------------------------------------------
    # Handler de notification Discord
    # -------------------------------------------------------------------------
    def notify_discord_on_failure(flow, flow_run, state):
        """Envoie une notification Discord en cas d'√©chec."""
        if not DISCORD_WEBHOOK_URL:
            print("‚ö†Ô∏è  DISCORD_WEBHOOK_URL non configur√©")
            return

        import urllib.request

        message = {
            "embeds": [{
                "title": "‚ùå Pipeline √âchou√© !",
                "description": f"**Flow:** {flow.name}\n**√âtat:** {state.name}",
                "color": 15158332,  # Rouge
                "fields": [
                    {"name": "Run ID", "value": str(flow_run.id)[:8], "inline": True},
                    {"name": "Heure", "value": datetime.now().strftime("%H:%M:%S"), "inline": True}
                ]
            }]
        }

        req = urllib.request.Request(
            DISCORD_WEBHOOK_URL,
            data=json.dumps(message).encode('utf-8'),
            headers={
                'Content-Type': 'application/json',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        )

        try:
            urllib.request.urlopen(req)
            print("üì® Notification Discord envoy√©e !")
        except Exception as e:
            print(f"‚ùå Erreur Discord : {e}")

    # -------------------------------------------------------------------------
    # TODO : Ajoutez on_failure=[notify_discord_on_failure] au flow
    # -------------------------------------------------------------------------
    @task
    def failing_task():
        """T√¢che qui √©choue volontairement pour tester les notifications."""
        raise Exception("√âchec simul√© pour tester les notifications !")

    @flow(name="notification-test-flow", log_prints=True, on_failure=[notify_discord_on_failure])
    def notification_test_flow():
        """Flow de test pour les notifications."""
        print("üß™ Test des notifications...")
        failing_task()

    # Ex√©cuter
    print("\nüìã Configuration des notifications :")
    print("")
    print("   DISCORD :")
    print("   1. Cr√©ez un serveur Discord ou utilisez un existant")
    print("   2. Param√®tres du serveur > Int√©grations > Webhooks > Nouveau")
    print("   3. Copiez l'URL du webhook")
    print("   4. D√©finissez : export DISCORD_WEBHOOK_URL='votre-url'")
    print("")
    print("   SLACK :")
    print("   1. Allez sur api.slack.com/apps et cr√©ez une app")
    print("   2. Activez 'Incoming Webhooks'")
    print("   3. Ajoutez un webhook pour votre channel")
    print("   4. D√©finissez : export SLACK_WEBHOOK_URL='votre-url'")
    print("")

    if DISCORD_WEBHOOK_URL or SLACK_WEBHOOK_URL:
        print("‚úÖ Webhook configur√© ! Lancement du test...\n")
        try:
            notification_test_flow()
        except Exception:
            print("\n‚úÖ Le flow a √©chou√© (comme pr√©vu).")
            print("   V√©rifiez Discord/Slack pour la notification !")
    else:
        print("‚ö†Ô∏è  Aucun webhook configur√©.")
        print("   D√©finissez DISCORD_WEBHOOK_URL ou SLACK_WEBHOOK_URL")
        print("   puis r√©ex√©cutez pour tester.\n")

        print("Exemple avec variable d'environnement :")
        print("   export DISCORD_WEBHOOK_URL='https://discord.com/api/webhooks/...'")
        print("   python Prefect_Exercises.py notif")

    # -------------------------------------------------------------------------
    # MINI-D√âFIS :
    # -------------------------------------------------------------------------
    #
    # D√âFI 7.1 : Ajoutez aussi on_completion pour √™tre notifi√© des succ√®s.
    #
    # D√âFI 7.2 : Personnalisez le message Discord avec plus de d√©tails
    #            (param√®tres utilis√©s, m√©triques obtenues...).
    #
    # D√âFI 7.3 : Cr√©ez un handler qui envoie un email (avec smtplib).
    # -------------------------------------------------------------------------


# =============================================================================
# √âTAPE 8 : D√âPLOIEMENT DOCKER - Ex√©cution dans le Worker
# =============================================================================
#
# OBJECTIF : D√©ployer le pipeline pour qu'il s'ex√©cute dans le worker Docker
#
# CONCEPTS :
#   - flow.serve() : Ex√©cute le flow LOCALEMENT (dans votre terminal)
#   - flow.deploy() : Enregistre le flow pour ex√©cution dans un WORKER DOCKER
#
# DIFF√âRENCE CL√â :
#   - serve() bloque votre terminal (Ctrl+C pour arr√™ter)
#   - deploy() lib√®re votre terminal (le worker Docker ex√©cute le flow)
#
# ARCHITECTURE PRODUCTION :
#   Votre code ‚Üí Prefect Server ‚Üí Work Pool ‚Üí Worker Docker ‚Üí Ex√©cution
#
# =============================================================================

def run_worker_demo():
    """
    √âTAPE 8 : D√©ployer vers le worker Docker

    Cette √©tape d√©montre la diff√©rence entre serve() et deploy().
    Le flow sera ex√©cut√© par le worker Docker, pas localement !
    """
    from prefect import flow, task
    from prefect.tasks import task_input_hash

    print("=" * 70)
    print("√âTAPE 8 : D√âPLOIEMENT DOCKER - Le worker ex√©cute le flow")
    print("=" * 70)

    print("""
üìã DIFF√âRENCE serve() vs deploy() :

  serve()  ‚Üí Le flow s'ex√©cute ICI (votre terminal)
             Terminal BLOQU√â jusqu'√† Ctrl+C
             Simple, bon pour apprendre

  deploy() ‚Üí Le flow s'ex√©cute dans le WORKER DOCKER
             Terminal LIB√âR√â imm√©diatement
             Architecture de production

""")

    print("=" * 70)
    print("üéØ D√âPLOIEMENT VERS LE WORKER DOCKER...")
    print("=" * 70)

    # -------------------------------------------------------------------------
    # SOLUTION : Utiliser from_source() pour d√©ployer vers un work pool
    #
    # Pour un work pool de type "process", on doit indiquer o√π se trouve
    # le code que le worker doit ex√©cuter. On utilise le flow
    # production_pipeline du fichier Prefect_Workshop.py qui est d√©fini
    # au niveau module.
    # -------------------------------------------------------------------------
    from prefect import flow

    # Charger le flow depuis le Workshop via from_source()
    deployed_flow = flow.from_source(
        source=str(PROJECT_ROOT),
        entrypoint="pipelines/workshop/02_prefect/Prefect_Workshop.py:production_pipeline"
    )

    deployment_id = deployed_flow.deploy(
        name="worker-training-exercises",
        work_pool_name="default-pool",
        cron="*/5 * * * *",  # Toutes les 5 minutes
        tags=["workshop", "ml", "worker-demo", "exercises"],
        description="Pipeline ex√©cut√© par le worker Docker - toutes les 5 minutes",
        parameters={
            "n_estimators": 100,
            "max_depth": 10,
            "experiment_name": "workshop-exercises-worker",
            "model_name": "churn-predictor-exercises"
        }
    )

    print()
    print("=" * 70)
    print("‚úÖ D√âPLOIEMENT R√âUSSI !")
    print("=" * 70)
    print(f"""
Le flow est maintenant d√©ploy√© vers le worker Docker.

PROCHAINES √âTAPES :

1. Ouvrir l'interface Prefect : http://localhost:4200
   ‚Üí Aller dans Deployments
   ‚Üí Trouver "churn-prediction-pipeline/worker-training-exercises"

2. D√©clencher manuellement :
   ‚Üí Cliquer sur "Quick Run"
   ‚Üí Observer l'ex√©cution dans l'onglet "Runs"

3. Voir les logs du worker :
   docker-compose logs -f prefect-worker

4. V√©rifier MLflow : http://localhost:5000

5. Pour supprimer le d√©ploiement :
   ‚Üí Interface Prefect > Deployments > Delete

COMPARAISON :
  - 'deploy' (serve)  : Ex√©cution locale, terminal bloqu√©
  - 'worker-demo'     : Ex√©cution Docker, terminal libre ‚Üê VOUS √äTES ICI
""")

    return deployment_id


# =============================================================================
# MAIN
# =============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("üéì PREFECT EXERCISES - Approche Fil Rouge")
    print("   Transformez un script ML en pipeline de production !")
    print("=" * 70)

    etape = sys.argv[1] if len(sys.argv) > 1 else "help"

    if etape == "etape1":
        run_etape1()

    elif etape == "etape2":
        run_etape2()

    elif etape == "etape3":
        run_etape3()

    elif etape == "etape4":
        run_etape4()

    elif etape == "etape5":
        run_etape5()

    elif etape == "deploy":
        run_deploy()

    elif etape == "notif":
        run_notifications()

    elif etape == "worker-demo":
        run_worker_demo()

    else:
        print("""
üéØ OBJECTIF : Transformer un script ML classique en pipeline de production

Le code de d√©part (fonctions load_churn_data, preprocess_data, train_model)
est un pipeline ML typique. Votre mission : le "blinder" progressivement
avec tous les patterns d'orchestration professionnels !

PROGRESSION :
  etape1      Tasks & Flows      ‚Üí Transformer les fonctions en t√¢ches
  etape2      R√©silience         ‚Üí Ajouter des r√©essais sur le chargement
  etape3      Efficacit√©         ‚Üí Cacher le preprocessing co√ªteux
  etape4      MLflow + Pipeline  ‚Üí Tracker avec sklearn Pipeline ‚≠ê
  etape5      Sous-flows         ‚Üí Organiser en modules r√©utilisables
  deploy      Automatisation     ‚Üí Planifier l'ex√©cution automatique (serve)
  worker-demo D√©ploiement Docker ‚Üí Ex√©cuter dans le worker Docker (deploy)
  notif       Notifications      ‚Üí Alertes Discord/Slack (bonus)

PR√âREQUIS :
  1. docker-compose ps                  # V√©rifier que Docker tourne
  2. Copier .env.example vers .env      # Config (une seule fois)

UTILISATION :
  python Prefect_Exercises.py etape1      # Commencez ici !
  python Prefect_Exercises.py etape2      # Puis continuez...
  python Prefect_Exercises.py etape3
  python Prefect_Exercises.py etape4      # MLflow + sklearn Pipeline
  python Prefect_Exercises.py etape5
  python Prefect_Exercises.py deploy      # Automatisation avec serve()
  python Prefect_Exercises.py worker-demo # Ex√©cution dans le worker Docker
  python Prefect_Exercises.py notif       # Bonus : alertes

R√âF√âRENCE :
  Consultez Prefect_Workshop.py pour voir les solutions compl√®tes.

INTERFACES :
  Prefect : http://localhost:4200
  MLflow  : http://localhost:5000
""")
