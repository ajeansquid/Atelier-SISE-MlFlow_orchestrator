# =============================================================================
# Atelier Prefect - Orchestration de Workflows ML
# =============================================================================
#
# Cet atelier enseigne l'ORCHESTRATION √† travers des cas d'usage ML.
#
# SECTIONS :
#   Partie 1 : Tasks & Flows - Les bases
#   Partie 2 : R√©silience - R√©essais, gestion d'erreurs
#   Partie 3 : Efficacit√© - Cache, ex√©cution parall√®le
#   Partie 4 : Flexibilit√© - Param√®tres, sous-flows
#   Partie 5 : Pipeline Complet - Tout avec MLflow
#   Partie 6 : AUTOMATISATION - D√©ployer, planifier, regarder l'ex√©cution !
#   Partie 7 : NOTIFICATIONS - Alertes Discord/Slack sur √©chec
#
# -----------------------------------------------------------------------------
# CONFIGURATION
# -----------------------------------------------------------------------------
#
# 1. D√©marrer la stack d'orchestration :
#      docker-compose up -d
#
# 2. Acc√©der aux interfaces :
#      - Prefect: http://localhost:4200 (flows, d√©ploiements, ex√©cutions)
#      - MLflow:  http://localhost:5000 (exp√©rimentations, mod√®les)
#
# 3. Ex√©cuter les parties de l'atelier :
#      python pipelines/workshop/02_prefect/Prefect_Workshop.py part1
#      ...
#      python pipelines/workshop/02_prefect/Prefect_Workshop.py deploy
#
# =============================================================================

import sys
import os
from pathlib import Path

from prefect import flow, task, serve
from prefect.tasks import task_input_hash
from prefect.client.orchestration import get_client
from datetime import timedelta, datetime
import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score
import time
import random
import os
import asyncio

# Imports pour les notifications (Partie 7)
# Note : httpx est une d√©pendance de Prefect, donc d√©j√† install√©
try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    print("‚ö†Ô∏è httpx non disponible - les notifications webhook ne fonctionneront pas")

# -----------------------------------------------------------------------------
# Configuration
# -----------------------------------------------------------------------------
# Charger le fichier .env pour la configuration locale (PREFECT_API_URL, MLFLOW_TRACKING_URI)
from dotenv import load_dotenv
PROJECT_ROOT = Path(__file__).resolve().parents[3]
load_dotenv(PROJECT_ROOT / ".env")

# Lors de l'ex√©cution dans Docker, ces variables d'environnement sont d√©finies par docker-compose
PROJECT_ROOT = Path(os.getenv("PROJECT_ROOT", str(PROJECT_ROOT)))
DATA_PATH = PROJECT_ROOT / "data" / "customer_data.csv"
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
PREFECT_API_URL = os.getenv("PREFECT_API_URL", "http://localhost:4200/api")

FEATURE_COLS = [
    'recency_days', 'frequency', 'monetary_value', 'avg_order_value',
    'days_since_signup', 'total_orders', 'support_tickets', 'age'
]


# =============================================================================
# PARTIE 1 : TASKS & FLOWS - Les Bases
# =============================================================================
#
# PROBL√àME ML :
#   Votre notebook ex√©cute les cellules s√©quentiellement. Si la cellule 5 √©choue,
#   vous relancez tout. Il n'y a pas de structure, pas de visibilit√©, pas de r√©utilisabilit√©.
#
# SOLUTION D'ORCHESTRATION :
#   D√©composer votre code en TASKS (responsabilit√©s uniques) et FLOWS (pipelines).
#   Chaque t√¢che est ind√©pendante, logg√©e, et peut √™tre r√©essay√©e.
#
# =============================================================================

@task
def load_data() -> pd.DataFrame:
    """
    Une TASK est une unit√© de travail unique.

    Points cl√©s :
    - Le d√©corateur @task transforme une fonction en t√¢che orchestr√©e
    - Les valeurs de retour circulent vers la t√¢che suivante (pas besoin d'I/O fichier !)
    - Prefect loggue automatiquement : heure de d√©but, dur√©e, succ√®s/√©chec
    """
    print("Chargement des donn√©es clients...")

    if DATA_PATH.exists():
        df = pd.read_csv(DATA_PATH)
    else:
        # G√©n√©rer des donn√©es synth√©tiques
        np.random.seed(42)
        n = 1000
        df = pd.DataFrame({
            'customer_id': range(1, n + 1),
            'recency_days': np.random.exponential(30, n).astype(int),
            'frequency': np.random.poisson(5, n),
            'monetary_value': np.random.exponential(500, n),
            'avg_order_value': np.random.exponential(100, n),
            'days_since_signup': np.random.randint(30, 1000, n),
            'total_orders': np.random.poisson(8, n),
            'support_tickets': np.random.poisson(2, n),
            'age': np.random.randint(18, 70, n),
            'churned': np.random.binomial(1, 0.3, n)
        })

    print(f"Charg√© {len(df)} lignes")
    return df


@task
def engineer_features(df: pd.DataFrame) -> pd.DataFrame:
    """T√¢che de feature engineering."""
    print("Ing√©nierie des features...")
    df = df.copy()

    df['recency_frequency_ratio'] = df['recency_days'] / (df['frequency'] + 1)
    df['monetary_per_order'] = df['monetary_value'] / (df['total_orders'] + 1)

    df['r_score'] = pd.qcut(df['recency_days'], q=5, labels=[5, 4, 3, 2, 1]).astype(int)
    df['f_score'] = pd.qcut(df['frequency'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5]).astype(int)
    df['m_score'] = pd.qcut(df['monetary_value'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5]).astype(int)
    df['rfm_score'] = df['r_score'] + df['f_score'] + df['m_score']

    print(f"Cr√©√© {len(df.columns)} features")
    return df


@flow(name="basic-pipeline", log_prints=True)
def basic_pipeline():
    """
    Un FLOW orchestre les t√¢ches.

    Appelez les t√¢ches comme des fonctions normales - les donn√©es circulent via les valeurs de retour.
    """
    raw_data = load_data()
    features = engineer_features(raw_data)
    print(f"Pipeline termin√© ! Forme : {features.shape}")
    return features


# =============================================================================
# PARTIE 2 : R√âSILIENCE - Gestion des √âchecs
# =============================================================================

@task(retries=3, retry_delay_seconds=5)
def load_from_unreliable_api() -> pd.DataFrame:
    """
    Les R√âESSAIS g√®rent les √©checs transitoires automatiquement.

    Cette t√¢che va :
    1. Essayer de s'ex√©cuter
    2. Si elle √©choue, attendre 5 secondes
    3. R√©essayer (√† nouveau jusqu'√† 3 fois)
    """
    if random.random() < 0.5:
        print("API call failed! (simulated)")
        raise ConnectionError("API temporarily unavailable")

    print("Appel API r√©ussi !")
    return load_data()


@task(retries=3, retry_delay_seconds=[10, 30, 60])
def load_with_backoff() -> pd.DataFrame:
    """
    BACKOFF EXPONENTIEL : Attendre plus longtemps entre chaque r√©essai.
    Premier √©chec : 10s, Deuxi√®me : 30s, Troisi√®me : 60s
    """
    if random.random() < 0.7:
        raise ConnectionError("Rate limited!")
    return load_data()


# =============================================================================
# PARTIE 3 : EFFICACIT√â - Cache & Parall√©lisme
# =============================================================================

@task(cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))
def expensive_feature_engineering(df: pd.DataFrame) -> pd.DataFrame:
    """
    CACHE : Sauter le recalcul si les entr√©es n'ont pas chang√©.
    """
    print("Ex√©cution du feature engineering co√ªteux...")
    time.sleep(2)  # Simuler un calcul co√ªteux
    df = df.copy()
    df['expensive_feature'] = df['monetary_value'] * df['frequency']
    print("Feature engineering termin√© !")
    return df


@task
def train_random_forest(X_train, y_train, X_test, y_test) -> dict:
    """Entra√Æner un mod√®le Random Forest."""
    print("Entra√Ænement Random Forest...")
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    accuracy = accuracy_score(y_test, model.predict(X_test))
    print(f"Pr√©cision Random Forest : {accuracy:.4f}")
    return {"name": "RandomForest", "model": model, "accuracy": accuracy}


@task
def train_gradient_boosting(X_train, y_train, X_test, y_test) -> dict:
    """Entra√Æner un mod√®le Gradient Boosting."""
    print("Entra√Ænement Gradient Boosting...")
    model = GradientBoostingClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    accuracy = accuracy_score(y_test, model.predict(X_test))
    print(f"Pr√©cision Gradient Boosting : {accuracy:.4f}")
    return {"name": "GradientBoosting", "model": model, "accuracy": accuracy}


@task
def select_best_model(results: list) -> dict:
    """S√©lectionner le mod√®le avec la meilleure pr√©cision."""
    best = max(results, key=lambda x: x["accuracy"])
    print(f"Meilleur mod√®le : {best['name']} avec pr√©cision {best['accuracy']:.4f}")
    return best


@flow(name="parallel-training", log_prints=True)
def parallel_training_flow():
    """
    EX√âCUTION PARALL√àLE : Entra√Æner plusieurs mod√®les simultan√©ment.
    """
    df = load_data()
    df = engineer_features(df)

    X = df[FEATURE_COLS + ['rfm_score']]
    y = df['churned']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Ceux-ci s'ex√©cutent en parall√®le - pas de d√©pendance entre eux !
    rf_result = train_random_forest(X_train, y_train, X_test, y_test)
    gb_result = train_gradient_boosting(X_train, y_train, X_test, y_test)

    best = select_best_model([rf_result, gb_result])
    return best


# =============================================================================
# PARTIE 4 : FLEXIBILIT√â - Param√®tres & Sous-flows
# =============================================================================

@task
def train_with_params(df: pd.DataFrame, n_estimators: int, max_depth: int) -> dict:
    """T√¢che qui accepte des hyperparam√®tres."""
    X = df[FEATURE_COLS + ['rfm_score']]
    y = df['churned']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    model.fit(X_train, y_train)

    accuracy = accuracy_score(y_test, model.predict(X_test))
    f1 = f1_score(y_test, model.predict(X_test))

    print(f"Mod√®le : n_estimators={n_estimators}, max_depth={max_depth}, precision={accuracy:.4f}")
    return {"model": model, "params": {"n_estimators": n_estimators, "max_depth": max_depth}, "metrics": {"accuracy": accuracy, "f1": f1}}


@flow(name="parameterized-training", log_prints=True)
def parameterized_training_flow(n_estimators: int = 100, max_depth: int = 10):
    """
    FLOW PARAM√âTR√â : Configurer sans changer le code.
    """
    df = load_data()
    df = engineer_features(df)
    result = train_with_params(df, n_estimators, max_depth)
    return result


@flow(name="data-preparation", log_prints=True)
def data_preparation_subflow() -> pd.DataFrame:
    """SOUS-FLOW : Pr√©paration de donn√©es r√©utilisable."""
    df = load_data()
    df = engineer_features(df)
    return df


@flow(name="training-subflow", log_prints=True)
def training_subflow(df: pd.DataFrame, n_estimators: int = 100) -> dict:
    """Entra√Ænement comme sous-flow s√©par√©."""
    return train_with_params(df, n_estimators=n_estimators, max_depth=10)


# =============================================================================
# PARTIE 5 : PIPELINE COMPLET AVEC MLFLOW
# =============================================================================

@task(retries=3, retry_delay_seconds=5)
def load_data_with_retry() -> pd.DataFrame:
    """Chargement de donn√©es avec logique de r√©essai."""
    return load_data()


@task(cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))
def engineer_features_cached(df: pd.DataFrame) -> pd.DataFrame:
    """Feature engineering avec cache."""
    return engineer_features(df)


@task
def train_with_mlflow(df: pd.DataFrame, n_estimators: int, max_depth: int, experiment_name: str) -> dict:
    """
    T√¢che d'entra√Ænement avec int√©gration MLflow.

    Prefect g√®re : r√©essais, cache, planification
    MLflow g√®re : suivi des exp√©rimentations, versionnage des mod√®les
    """
    feature_cols = FEATURE_COLS + ['rfm_score']
    X = df[feature_cols]
    y = df['churned']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    mlflow.set_experiment(experiment_name)

    with mlflow.start_run(run_name=f"prefect-{datetime.now().strftime('%H%M%S')}") as run:
        mlflow.log_params({
            "n_estimators": n_estimators,
            "max_depth": max_depth,
            "orchestrator": "prefect",
            "n_features": len(feature_cols)
        })

        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
        model.fit(X_train, y_train)

        y_pred = model.predict(X_test)
        metrics = {"accuracy": accuracy_score(y_test, y_pred), "f1": f1_score(y_test, y_pred)}
        mlflow.log_metrics(metrics)
        mlflow.sklearn.log_model(model, name="model")

        print(f"MLflow run: {run.info.run_id}")
        print(f"Accuracy: {metrics['accuracy']:.4f}")

    return {"model": model, "run_id": run.info.run_id, "metrics": metrics}


@task
def register_model(run_id: str, model_name: str) -> str:
    """Enregistrer le mod√®le dans le registre MLflow."""
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    model_uri = f"runs:/{run_id}/model"
    result = mlflow.register_model(model_uri, model_name)
    print(f"Enregistr√© {model_name} version {result.version}")
    return result.version


@task
def generate_predictions(model, df: pd.DataFrame) -> pd.DataFrame:
    """G√©n√©rer des pr√©dictions pour tous les clients."""
    feature_cols = FEATURE_COLS + ['rfm_score']
    X = df[feature_cols]

    predictions = model.predict(X)
    probabilities = model.predict_proba(X)[:, 1]

    result = pd.DataFrame({
        'customer_id': df['customer_id'],
        'churn_predicted': predictions,
        'churn_probability': probabilities,
        'predicted_at': datetime.now()
    })

    high_risk = (probabilities > 0.7).sum()
    print(f"G√©n√©r√© {len(result)} pr√©dictions ({high_risk} √† haut risque)")
    return result


@task
def save_predictions(predictions: pd.DataFrame, output_path: str | Path) -> str:
    """Sauvegarder les pr√©dictions en CSV."""
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    predictions.to_csv(output_path, index=False)
    print(f"Sauvegard√© dans {output_path}")
    return output_path


@flow(name="churn-prediction-pipeline", log_prints=True)
def production_pipeline(
    n_estimators: int = 100,
    max_depth: int = 10,
    experiment_name: str = "workshop-prefect",
    model_name: str = "churn-predictor"
):
    """
    PIPELINE ML DE PRODUCTION

    Combine tous les patterns :
    - R√âESSAIS sur le chargement de donn√©es
    - CACHE sur le feature engineering
    - PARAM√àTRES pour les hyperparam√®tres
    - MLFLOW pour le suivi

    Ce flow peut √™tre :
    - Ex√©cut√© manuellement : production_pipeline()
    - D√©ploy√© avec planification : Voir Partie 6
    """
    print("=" * 60)
    print("PIPELINE DE PR√âDICTION DE CHURN")
    print(f"Heure : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # Donn√©es
    df = load_data_with_retry()
    df = engineer_features_cached(df)

    # Entra√Ænement
    result = train_with_mlflow(df, n_estimators, max_depth, experiment_name)

    # Enregistrement
    version = register_model(result["run_id"], model_name)

    # Inf√©rence
    predictions = generate_predictions(result["model"], df)
    output_path = PROJECT_ROOT / "data" / "predictions_workshop.csv"
    save_predictions(predictions, output_path)

    print("\n" + "=" * 60)
    print("PIPELINE TERMIN√â")
    print("=" * 60)
    print(f"Mod√®le : {model_name} v{version}")
    print(f"Pr√©cision : {result['metrics']['accuracy']:.4f}")
    print(f"Pr√©dictions : {len(predictions)}")

    return {"model_version": version, "metrics": result["metrics"], "predictions_count": len(predictions)}


# =============================================================================
# PARTIE 6 : AUTOMATISATION - D√©ployer & Planifier
# =============================================================================
#
# C'est ici que l'ORCHESTRATION devient une vraie automatisation !
#
# Jusqu'√† pr√©sent, nous avons ex√©cut√© les flows manuellement. En production :
# - Les flows s'ex√©cutent selon des PLANIFICATIONS (r√©entra√Ænement quotidien, pr√©dictions horaires)
# - Un WORKER ex√©cute les flows (dans Docker)
# - Vous SURVEILLEZ dans l'interface (voir les ex√©cutions, √©checs, logs)
#
# =============================================================================

def deploy_with_schedule():
    """
    D√âPLOYER le pipeline avec une planification.

    Ceci cr√©e un D√âPLOIEMENT :
    - Enregistre le flow aupr√®s du serveur Prefect
    - Configure une planification (toutes les 2 minutes pour la d√©mo)
    - Le Worker le r√©cup√®re et l'ex√©cute

    Apr√®s avoir ex√©cut√© ceci :
    1. Ouvrir l'interface Prefect : http://localhost:4200
    2. Aller dans D√©ploiements
    3. Voir "churn-prediction-pipeline/scheduled-training"
    4. Observer les ex√©cutions appara√Ætre toutes les 2 minutes !
    5. V√©rifier l'interface MLflow : http://localhost:5000 pour les nouvelles exp√©rimentations

    POUR ARR√äTER :
    - Terminal : Ctrl+C (arr√™te le processus serve())
    - Interface Prefect : Deployments > scheduled-training > Pause ou Delete
    """
    print("=" * 60)
    print("D√âPLOIEMENT DU PIPELINE AVEC PLANIFICATION")
    print("=" * 60)
    print("\nCeci va :")
    print("1. Enregistrer le flow aupr√®s du serveur Prefect")
    print("2. Configurer une planification (toutes les 2 minutes)")
    print("3. Le Worker l'ex√©cutera automatiquement")
    print("\nApr√®s le d√©ploiement :")
    print(f"  - Interface Prefect: http://localhost:4200")
    print(f"  - Interface MLflow:  {MLFLOW_TRACKING_URI}")
    print("\nPOUR ARR√äTER :")
    print("  - Terminal : Ctrl+C")
    print("  - Interface : Deployments > Pause ou Delete")
    print("=" * 60)

    # D√©ployer en utilisant serve() - ceci ex√©cute le planificateur localement
    # En production, vous utiliseriez flow.deploy() avec un work pool
    production_pipeline.serve(
        name="scheduled-training",
        cron="*/2 * * * *",  # Toutes les 2 minutes (pour la d√©mo)
        tags=["workshop", "ml", "churn"],
        description="R√©entra√Ænement automatis√© du mod√®le de churn - s'ex√©cute toutes les 2 minutes",
        parameters={
            "n_estimators": 100,
            "max_depth": 10,
            "experiment_name": "workshop-automated",
            "model_name": "churn-predictor-auto"
        }
    )


def run_single():
    """Ex√©cuter le pipeline une fois (pour tester avant le d√©ploiement)."""
    print("Ex√©cution du pipeline une fois...")
    result = production_pipeline(
        n_estimators=100,
        max_depth=10,
        experiment_name="workshop-prefect",
        model_name="churn-predictor"
    )
    return result


def deploy_to_worker():
    """
    D√âPLOYER le pipeline vers le WORKER DOCKER.

    DIFF√âRENCE avec serve() :
    - serve() : Le flow s'ex√©cute dans VOTRE processus Python (local)
    - deploy() : Le flow s'ex√©cute dans le WORKER DOCKER (production)

    Apr√®s avoir ex√©cut√© ceci :
    1. Votre terminal est LIB√âR√â (pas de processus bloquant)
    2. Le worker Docker va ex√©cuter le flow
    3. Ouvrir l'interface Prefect : http://localhost:4200
    4. Aller dans Deployments > worker-training
    5. Cliquer sur "Quick Run" pour d√©clencher manuellement
    6. Ou attendre la planification (toutes les 5 minutes)

    POUR VOIR LES LOGS DU WORKER :
        docker-compose logs -f prefect-worker

    POUR SUPPRIMER LE D√âPLOIEMENT :
        Interface Prefect > Deployments > Delete
    """
    print("=" * 60)
    print("D√âPLOIEMENT VERS LE WORKER DOCKER")
    print("=" * 60)
    print("""
DIFF√âRENCE serve() vs deploy() :

  serve()  ‚Üí Le flow s'ex√©cute ICI (votre terminal)
             Ctrl+C arr√™te tout
             Simple, bon pour apprendre

  deploy() ‚Üí Le flow s'ex√©cute dans le WORKER DOCKER
             Votre terminal est lib√©r√©
             Architecture de production

""")
    print("D√©ploiement en cours...")
    print()

    # Cr√©er le d√©ploiement vers le work pool Docker
    # Le worker dans docker-compose √©coute "default-pool"
    # Note: Pour un work pool de type "process", on utilise from_source() pour
    # indiquer o√π se trouve le code que le worker doit ex√©cuter.
    deployment_id = production_pipeline.from_source(
        source=str(PROJECT_ROOT),  # Chemin vers le code source
        entrypoint="pipelines/workshop/02_prefect/Prefect_Workshop.py:production_pipeline"
    ).deploy(
        name="worker-training",
        work_pool_name="default-pool",
        cron="*/5 * * * *",  # Toutes les 5 minutes
        tags=["workshop", "ml", "worker-demo"],
        description="Pipeline ex√©cut√© par le worker Docker - toutes les 5 minutes",
        parameters={
            "n_estimators": 100,
            "max_depth": 10,
            "experiment_name": "workshop-worker",
            "model_name": "churn-predictor-worker"
        }
    )

    print()
    print("=" * 60)
    print("‚úÖ D√âPLOIEMENT R√âUSSI !")
    print("=" * 60)
    print(f"""
Le flow est maintenant d√©ploy√© vers le worker Docker.

PROCHAINES √âTAPES :

1. Ouvrir l'interface Prefect : http://localhost:4200
   ‚Üí Aller dans Deployments
   ‚Üí Trouver "churn-prediction-pipeline/worker-training"

2. D√©clencher manuellement :
   ‚Üí Cliquer sur "Quick Run"
   ‚Üí Observer l'ex√©cution dans l'onglet "Runs"

3. Voir les logs du worker :
   docker-compose logs -f prefect-worker

4. V√©rifier MLflow : http://localhost:5000
   ‚Üí Exp√©rience "workshop-worker"

5. Pour supprimer le d√©ploiement :
   ‚Üí Interface Prefect > Deployments > Delete

COMPARAISON :
  - 'deploy' (serve)  : Ex√©cution locale, terminal bloqu√©
  - 'worker-demo'     : Ex√©cution Docker, terminal libre ‚Üê VOUS √äTES ICI
""")

    return deployment_id


# =============================================================================
# PARTIE 7 : NOTIFICATIONS - Alertes Discord/Slack
# =============================================================================
#
# PROBL√àME :
#   Vous ne voulez pas surveiller vos pipelines 24h/24. Vous voulez √™tre
#   ALERT√â quand quelque chose √©choue (ou r√©ussit) !
#
# SOLUTION :
#   Prefect supporte les notifications via webhooks : Discord, Slack, Teams, etc.
#   Recevez un message sur votre serveur Discord quand un flow √©choue !
#
# =============================================================================

def send_discord_notification(webhook_url: str, message: str, username: str = "Prefect Bot"):
    """
    Envoyer une notification Discord via webhook.

    Pour cr√©er un webhook Discord :
    1. Aller dans les param√®tres de votre serveur Discord
    2. Int√©grations > Webhooks > Nouveau Webhook
    3. Copier l'URL du webhook

    L'URL ressemble √† : https://discord.com/api/webhooks/<id>/<token>
    """
    if not HTTPX_AVAILABLE:
        print("‚ö†Ô∏è httpx non disponible - notification non envoy√©e")
        return

    payload = {
        "username": username,
        "content": message
    }

    try:
        response = httpx.post(webhook_url, json=payload)
        response.raise_for_status()
        print(f"‚úÖ Notification Discord envoy√©e")
    except Exception as e:
        print(f"‚ùå Erreur envoi Discord : {e}")


def send_slack_notification(webhook_url: str, message: str):
    """
    Envoyer une notification Slack via webhook.

    Pour cr√©er un webhook Slack :
    1. Aller sur https://api.slack.com/apps
    2. Cr√©er une app > Incoming Webhooks > Activer
    3. Ajouter √† un channel et copier l'URL
    """
    if not HTTPX_AVAILABLE:
        print("‚ö†Ô∏è httpx non disponible - notification non envoy√©e")
        return

    payload = {"text": message}

    try:
        response = httpx.post(webhook_url, json=payload)
        response.raise_for_status()
        print(f"‚úÖ Notification Slack envoy√©e")
    except Exception as e:
        print(f"‚ùå Erreur envoi Slack : {e}")


def create_failure_handler(webhook_url: str, service: str = "discord"):
    """
    Cr√©er un handler d'√©chec pour un flow.

    Ce handler sera appel√© automatiquement quand le flow √©choue.
    """
    def handle_failure(flow, flow_run, state):
        message = f"""üö® **√âCHEC DU PIPELINE**

üìã Flow : `{flow.name}`
üî¢ Run ID : `{flow_run.id}`
‚ùå √âtat : `{state.name}`
‚è∞ Heure : `{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}`

Consultez l'interface Prefect pour plus de d√©tails : http://localhost:4200
"""
        if service == "discord":
            send_discord_notification(webhook_url, message)
        elif service == "slack":
            send_slack_notification(webhook_url, message)

    return handle_failure


def create_success_handler(webhook_url: str, service: str = "discord"):
    """
    Cr√©er un handler de succ√®s pour un flow.

    Optionnel : √™tre notifi√© aussi des succ√®s (utile pour le monitoring).
    """
    def handle_success(flow, flow_run, state):
        message = f"""‚úÖ **PIPELINE R√âUSSI**

üìã Flow : `{flow.name}`
üî¢ Run ID : `{flow_run.id}`
‚úÖ √âtat : `{state.name}`
‚è∞ Heure : `{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}`
"""
        if service == "discord":
            send_discord_notification(webhook_url, message)
        elif service == "slack":
            send_slack_notification(webhook_url, message)

    return handle_success


# Exemple de flow avec notifications
# NOTE : Remplacez VOTRE_WEBHOOK_URL par votre vrai webhook Discord/Slack !
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL", "")
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL", "")


@task
def task_that_might_fail(fail_probability: float = 0.5):
    """T√¢che qui peut √©chouer (pour tester les notifications)."""
    print(f"Ex√©cution de la t√¢che (probabilit√© d'√©chec : {fail_probability})")
    if random.random() < fail_probability:
        raise ValueError("üí• √âchec simul√© ! V√©rifiez vos notifications.")
    print("‚úÖ T√¢che r√©ussie !")
    return "success"


def run_notification_demo():
    """
    D√©monstration des notifications.

    INSTRUCTIONS :
    1. Cr√©ez un webhook Discord ou Slack
    2. D√©finissez la variable d'environnement :
       - Discord : export DISCORD_WEBHOOK_URL="https://discord.com/api/webhooks/..."
       - Slack : export SLACK_WEBHOOK_URL="https://hooks.slack.com/services/..."
    3. Ex√©cutez cette d√©mo avec : python Prefect_Workshop.py part7
    """
    print("=" * 60)
    print("PARTIE 7 : NOTIFICATIONS")
    print("=" * 60)

    # V√©rifier si un webhook est configur√©
    webhook_url = DISCORD_WEBHOOK_URL or SLACK_WEBHOOK_URL
    service = "discord" if DISCORD_WEBHOOK_URL else "slack"

    if not webhook_url:
        print("""
‚ö†Ô∏è  AUCUN WEBHOOK CONFIGUR√â !

Pour tester les notifications :

1. Cr√©er un webhook Discord :
   - Param√®tres serveur > Int√©grations > Webhooks > Nouveau
   - Copier l'URL

2. D√©finir la variable d'environnement :
   - Windows : set DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/...
   - Linux/Mac : export DISCORD_WEBHOOK_URL="https://discord.com/api/webhooks/..."

3. Relancer : python Prefect_Workshop.py part7

ALTERNATIVE : Mode simulation (sans vrai webhook)
""")
        # Mode simulation sans vrai webhook
        print("Mode simulation : on affiche ce qui serait envoy√©\n")

        @flow(name="notification-demo-simulation", log_prints=True)
        def demo_flow_simulation():
            print("üîî [SIMULATION] Notification de d√©marrage serait envoy√©e")
            result = task_that_might_fail(fail_probability=0.5)
            print("üîî [SIMULATION] Notification de succ√®s serait envoy√©e")
            return result

        try:
            demo_flow_simulation()
        except Exception as e:
            print(f"üîî [SIMULATION] Notification d'√©chec serait envoy√©e : {e}")

        return

    # Webhook configur√© - cr√©er les handlers
    failure_handler = create_failure_handler(webhook_url, service)
    success_handler = create_success_handler(webhook_url, service)

    @flow(
        name="notification-demo",
        log_prints=True,
        on_failure=[failure_handler],
        on_completion=[success_handler]  # Appel√© sur succ√®s ET √©chec
    )
    def demo_flow_with_notifications():
        """Flow avec notifications automatiques."""
        result = task_that_might_fail(fail_probability=0.3)
        return result

    print(f"\nWebhook {service.upper()} configur√© !")
    print("Ex√©cution du flow de d√©monstration...")
    print("Surveillez votre channel pour les notifications !\n")

    try:
        demo_flow_with_notifications()
        print("\n‚úÖ Flow termin√© avec succ√®s - notification envoy√©e !")
    except Exception as e:
        print(f"\n‚ùå Flow √©chou√© - notification d'√©chec envoy√©e !")


# =============================================================================
# NOTE P√âDAGOGIQUE : PATTERNS DE PR√âTRAITEMENT
# =============================================================================
#
# Les patterns de preprocessing (scaler.pkl, sklearn Pipeline) sont
# couverts en d√©tail dans les notebooks :
#   - 01b_mlflow_transition.ipynb (section 2.3)
#   - 02_mlflow_organized.ipynb (sections 8.1-8.6, comparaison compl√®te)
#
# Ce fichier se concentre sur l'ORCHESTRATION avec Prefect.
#
# R√âSUM√â DES MEILLEURES PRATIQUES :
#   - Utilisez sklearn Pipeline pour combiner scaler + mod√®le en UN artefact
#   - Loggez le pipeline complet dans MLflow : mlflow.sklearn.log_model(pipeline)
#   - √Ä l'inf√©rence : pipeline.predict(X) applique automatiquement le scaling
#
# =============================================================================


# =============================================================================
# MAIN
# =============================================================================

if __name__ == "__main__":
    import sys

    print("=" * 60)
    print("PREFECT WORKSHOP - Orchestrating ML Workflows")
    print("=" * 60)

    mode = sys.argv[1] if len(sys.argv) > 1 else "help"

    if mode == "part1":
        print("\nPART 1: Basic Tasks & Flows")
        print("-" * 40)
        result = basic_pipeline()
        print(f"Result shape: {result.shape}")

    elif mode == "part2":
        print("\nPART 2: Resilience (Retries)")
        print("-" * 40)

        @flow
        def retry_demo():
            return load_from_unreliable_api()

        try:
            retry_demo()
        except Exception as e:
            print(f"Failed after retries: {e}")

    elif mode == "part3":
        print("\nPARTIE 3 : Efficacit√© (Cache & Parall√©lisme)")
        print("-" * 40)
        result = parallel_training_flow()

    elif mode == "part4":
        print("\nPARTIE 4 : Flexibilit√© (Param√®tres & Sous-flows)")
        print("-" * 40)
        result = parameterized_training_flow(n_estimators=50)

    elif mode == "part5" or mode == "full":
        print("\nPARTIE 5 : Pipeline Complet avec MLflow")
        print("-" * 40)
        result = run_single()

    elif mode == "deploy":
        print("\nPARTIE 6 : D√©ployer avec Planification (AUTOMATISATION !)")
        print("-" * 40)
        print("\nCeci va d√©marrer un processus de longue dur√©e qui :")
        print("1. Enregistre le flow")
        print("2. L'ex√©cute toutes les 2 minutes")
        print("3. Loggue dans MLflow")
        print("\nAppuyez sur Ctrl+C pour arr√™ter.\n")
        deploy_with_schedule()

    elif mode == "worker-demo" or mode == "worker":
        print("\nBONUS : D√©ployer vers le Worker Docker")
        print("-" * 40)
        deploy_to_worker()

    elif mode == "part7" or mode == "notifications":
        run_notification_demo()

    else:
        print("""
ATELIER PREFECT - Orchestration ML
====================================

CONFIGURATION :
  docker-compose up -d

  Interfaces :
    Prefect: http://localhost:4200
    MLflow:  http://localhost:5000

MODES :
  part1           Tasks & Flows (bases)
  part2           R√©silience (r√©essais)
  part3           Efficacit√© (cache, parall√©lisme)
  part4           Flexibilit√© (param√®tres, sous-flows)
  part5           Pipeline Complet avec MLflow
  deploy          AUTOMATISATION - Ex√©cution locale avec serve()
  worker-demo     PRODUCTION - Ex√©cution dans le worker Docker !
  part7           NOTIFICATIONS - Alertes Discord/Slack

D√âROULEMENT DE L'ATELIER :
  1. Ex√©cuter part1-part5 pour apprendre les patterns d'orchestration
  2. Ex√©cuter 'deploy' pour voir l'automatisation r√©elle
  3. Ex√©cuter 'worker-demo' pour d√©ployer vers le worker Docker
  4. Ex√©cuter 'part7' pour configurer les alertes
  5. Ouvrir l'interface Prefect pour observer les ex√©cutions
  6. Ouvrir l'interface MLflow pour voir les exp√©rimentations

NOTE : Les patterns de preprocessing (scaler.pkl, sklearn Pipeline) sont
       couverts dans les notebooks (01b, 02_mlflow_organized).

NOTIFICATIONS (mode part7) :
  - Cr√©er un webhook Discord ou Slack
  - D√©finir la variable : DISCORD_WEBHOOK_URL ou SLACK_WEBHOOK_URL
  - Recevoir des alertes automatiques sur √©chec !

AUTOMATISATION :
  deploy       ‚Üí serve() : ex√©cution locale, terminal bloqu√©
  worker-demo  ‚Üí deploy() : ex√©cution Docker, terminal libre (production)

  Comparaison :
    - 'deploy' utilise serve() - le flow tourne dans VOTRE terminal
    - 'worker-demo' utilise deploy() - le flow tourne dans le WORKER DOCKER

Exemple :
  python Prefect_Workshop.py part1       # Apprendre les bases
  python Prefect_Workshop.py part5       # Pipeline complet avec MLflow
  python Prefect_Workshop.py deploy      # Local (Ctrl+C pour arr√™ter)
  python Prefect_Workshop.py worker-demo # Docker (terminal lib√©r√©)
  python Prefect_Workshop.py part7       # Notifications Discord/Slack
""")
