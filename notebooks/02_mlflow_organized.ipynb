{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Prédiction du Churn Client - MLflow Organisé\n\n## Du Chaos à la Reproductibilité\n\nDans le notebook précédent (`01_messy_notebook.ipynb`), nous avons vécu le chaos typique du data science :\n- Plusieurs expériences sans tracking\n- Métriques éparpillées dans les cellules\n- \"C'était lequel le meilleur modèle déjà ?\"\n- Pas de versioning ni de reproductibilité\n\n**Dans ce notebook, nous résolvons ces problèmes avec MLflow :**\n- Tracking d'expériences (paramètres, métriques, artefacts)\n- Versioning de modèles et registry\n- Comparaison facile entre les runs\n- Résultats reproductibles\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Setup et Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# NOUVEAU : imports MLflow\nimport mlflow\nimport mlflow.sklearn\nfrom mlflow.tracking import MlflowClient\n\nprint(f\"Version MLflow : {mlflow.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Configuration MLflow\n\nMLflow doit savoir :\n1. **Où stocker les données de tracking** (tracking URI)\n2. **Dans quelle expérience logger** (nom de l'expérience)\n\n**Pour ce workshop :** Démarrez d'abord le serveur MLflow avec `cd docker && docker-compose up -d`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configurer le tracking MLflow\n# Se connecter au serveur MLflow hébergé dans Docker (démarrer avec : docker-compose up -d)\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n\n# Créer ou récupérer l'expérience\nexperiment_name = \"customer-churn-prediction\"\nmlflow.set_experiment(experiment_name)\n\nprint(f\"Tracking URI : {mlflow.get_tracking_uri()}\")\nprint(f\"Expérience : {experiment_name}\")\n\n# Tester la connexion avec un simple ping\ntry:\n    import requests\n    response = requests.get(f\"{mlflow.get_tracking_uri()}/health\")\n    print(f\"✅ Serveur MLflow répond : {response.status_code}\")\nexcept Exception as e:\n    print(f\"❌ Impossible de se connecter au serveur MLflow : {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Charger les données\n\nOn charge les données clients depuis le CSV. Exécutez `python generate_sample_data.py` d'abord si le fichier n'existe pas."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Charger les données clients depuis le CSV\ndf = pd.read_csv('../data/customer_data.csv')\n\nprint(f\"Dimensions du dataset : {df.shape}\")\nprint(f\"Taux de churn : {df['churned'].mean():.2%}\")\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Feature Engineering\n\nMêmes features qu'avant - nous allons tracker quelles features nous utilisons dans MLflow."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def engineer_features(df):\n    \"\"\"Créer des features engineerées - encapsulé dans une fonction pour la reproductibilité\"\"\"\n    df = df.copy()\n    \n    # Features de ratio\n    df['recency_frequency_ratio'] = df['recency_days'] / (df['frequency'] + 1)\n    df['monetary_per_order'] = df['monetary_value'] / (df['total_orders'] + 1)\n    df['order_frequency'] = df['total_orders'] / (df['days_since_signup'] + 1)\n    df['support_per_order'] = df['support_tickets'] / (df['total_orders'] + 1)\n    \n    # Score RFM\n    df['r_score'] = pd.qcut(df['recency_days'], q=5, labels=[5, 4, 3, 2, 1]).astype(int)\n    df['f_score'] = pd.qcut(df['frequency'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5]).astype(int)\n    df['m_score'] = pd.qcut(df['monetary_value'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5]).astype(int)\n    df['rfm_score'] = df['r_score'] + df['f_score'] + df['m_score']\n    \n    return df\n\n# Appliquer le feature engineering\ndf = engineer_features(df)\nprint(f\"Features après engineering : {df.shape[1]} colonnes\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Définir les colonnes de features (on va logger ça dans MLflow)\nFEATURE_COLS = [\n    'recency_days', 'frequency', 'monetary_value', 'avg_order_value',\n    'days_since_signup', 'total_orders', 'support_tickets', 'age',\n    'recency_frequency_ratio', 'monetary_per_order', 'order_frequency',\n    'support_per_order', 'rfm_score'\n]\n\nX = df[FEATURE_COLS]\ny = df['churned']\n\n# Séparation train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Échantillons d'entraînement : {len(X_train)}\")\nprint(f\"Échantillons de test : {len(X_test)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Tracking d'expériences MLflow\n\nMaintenant la magie opère ! Au lieu d'afficher les métriques et de les oublier, nous **loggons tout dans MLflow**.\n\n### Concepts clés MLflow :\n- **Run** : Une exécution unique de votre code (une expérience)\n- **Parameters** : Les entrées de votre modèle (hyperparamètres)\n- **Metrics** : Les sorties/résultats (accuracy, F1, etc.)\n- **Artifacts** : Les fichiers (modèles, graphiques, échantillons de données)\n- **Tags** : Les métadonnées (auteur, version, notes)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.1 Exemple de logging manuel\n\n**Ce qu'on logge :**\n- **Parameters** : Hyperparamètres, nombre de features, tailles train/test\n- **Metrics** : Accuracy, precision, recall, F1\n- **Artifacts** : Modèle, matrice de confusion, importance des features\n- **Evaluation Tables** : Tableau détaillé des prédictions\n- **Dataset** : Info sur les données d'entraînement (comme autolog fait)\n\n**Note :** Assurez-vous que l'autologging est désactivé avant le logging manuel pour éviter les conflits."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# S'assurer que l'autologging est désactivé pour éviter les conflits\nmlflow.sklearn.autolog(disable=True)\n\nimport tempfile\nimport os\n\ndef train_and_log_model(model, model_name, X_train, X_test, y_train, y_test, params=None):\n    \"\"\"\n    Entraîner un modèle et logger tout dans MLflow.\n    Inclut : paramètres, métriques, modèle, artefacts, résultats d'évaluation et dataset.\n    \"\"\"\n    with mlflow.start_run(run_name=model_name):\n        # Logger les tags (métadonnées)\n        mlflow.set_tag(\"model_type\", model_name)\n        mlflow.set_tag(\"author\", \"workshop\")\n        mlflow.set_tag(\"stage\", \"experimentation\")\n        \n        # === Logger le dataset (comme autolog fait) ===\n        train_data = X_train.copy()\n        train_data['target'] = y_train.values\n        dataset = mlflow.data.from_pandas(\n            train_data, \n            source=\"../data/customer_data.csv\",\n            name=\"customer_churn_training\",\n            targets=\"target\"\n        )\n        mlflow.log_input(dataset, context=\"training\")\n        \n        # Logger les paramètres\n        if params:\n            mlflow.log_params(params)\n        mlflow.log_param(\"n_features\", len(FEATURE_COLS))\n        mlflow.log_param(\"train_size\", len(X_train))\n        mlflow.log_param(\"test_size\", len(X_test))\n        \n        # Entraîner le modèle\n        model.fit(X_train, y_train)\n        \n        # Faire des prédictions\n        y_pred = model.predict(X_test)\n        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n        \n        # Calculer et logger les métriques\n        metrics = {\n            \"accuracy\": accuracy_score(y_test, y_pred),\n            \"precision\": precision_score(y_test, y_pred),\n            \"recall\": recall_score(y_test, y_pred),\n            \"f1\": f1_score(y_test, y_pred)\n        }\n        mlflow.log_metrics(metrics)\n        \n        # Logger les résultats d'évaluation en CSV avec tempfile (pas de fichier local laissé)\n        eval_df = pd.DataFrame({\n            'actual': y_test.values,\n            'predicted': y_pred,\n            'probability': y_proba if y_proba is not None else [None] * len(y_pred),\n            'correct': (y_test.values == y_pred).astype(int)\n        })\n        with tempfile.TemporaryDirectory() as tmpdir:\n            csv_path = os.path.join(tmpdir, \"evaluation_results.csv\")\n            eval_df.to_csv(csv_path, index=False)\n            mlflow.log_artifact(csv_path)\n        \n        # Logger le modèle\n        mlflow.sklearn.log_model(model, name=\"model\")\n        \n        # Logger l'importance des features (si disponible) - avec log_figure (pas de fichier local)\n        if hasattr(model, 'feature_importances_'):\n            fig, ax = plt.subplots(figsize=(10, 6))\n            feat_imp = pd.Series(model.feature_importances_, index=FEATURE_COLS)\n            feat_imp.sort_values().plot(kind='barh', ax=ax)\n            ax.set_title(f'Importance des Features - {model_name}')\n            plt.tight_layout()\n            mlflow.log_figure(fig, \"feature_importance.png\")\n            plt.close()\n        \n        # Logger la matrice de confusion - avec log_figure (pas de fichier local)\n        cm = confusion_matrix(y_test, y_pred)\n        fig, ax = plt.subplots(figsize=(6, 4))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n        ax.set_xlabel('Prédit')\n        ax.set_ylabel('Réel')\n        ax.set_title(f'Matrice de Confusion - {model_name}')\n        plt.tight_layout()\n        mlflow.log_figure(fig, \"confusion_matrix.png\")\n        plt.close()\n        \n        run_id = mlflow.active_run().info.run_id\n        print(f\"Modèle : {model_name} | Accuracy : {metrics['accuracy']:.4f} | F1 : {metrics['f1']:.4f}\")\n        print(f\"Loggé : modèle, dataset, matrice de confusion, CSV d'évaluation\")\n        \n        return run_id, metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.2 Lancer les expériences"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Artefacts de prétraitement\n\n**Important :** Les modèles nécessitant un prétraitement (scaling, encoding) doivent sauvegarder ces objets comme artefacts !\n\n- Au moment de l'inférence, vous avez besoin du **même** scaler utilisé pendant l'entraînement\n- Sans ça, les prédictions seront incorrectes (mauvaise échelle = mauvais résultats)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Expérience 1 : Régression Logistique (avec artefact de prétraitement)\nimport joblib\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nwith mlflow.start_run(run_name=\"LogisticRegression\") as run:\n    # Logger les tags\n    mlflow.set_tag(\"model_type\", \"LogisticRegression\")\n    mlflow.set_tag(\"author\", \"workshop\")\n    mlflow.set_tag(\"stage\", \"experimentation\")\n    mlflow.set_tag(\"requires_scaling\", \"true\")\n    \n    # === Logger le dataset (comme autolog fait) ===\n    train_data = X_train.copy()\n    train_data['target'] = y_train.values\n    dataset = mlflow.data.from_pandas(\n        train_data,\n        source=\"../data/customer_data.csv\",\n        name=\"customer_churn_training\",\n        targets=\"target\"\n    )\n    mlflow.log_input(dataset, context=\"training\")\n    \n    # Sauvegarder le scaler comme artefact avec tempfile (pas de fichier local laissé)\n    with tempfile.TemporaryDirectory() as tmpdir:\n        scaler_path = os.path.join(tmpdir, \"scaler.pkl\")\n        joblib.dump(scaler, scaler_path)\n        mlflow.log_artifact(scaler_path, artifact_path=\"preprocessing\")\n    \n    # Logger les paramètres\n    mlflow.log_params({\n        \"max_iter\": 1000,\n        \"solver\": \"lbfgs\",\n        \"n_features\": len(FEATURE_COLS),\n        \"train_size\": len(X_train_scaled),\n        \"test_size\": len(X_test_scaled)\n    })\n    \n    # Entraîner le modèle\n    lr = LogisticRegression(random_state=42, max_iter=1000)\n    lr.fit(X_train_scaled, y_train)\n    \n    # Faire des prédictions\n    y_pred = lr.predict(X_test_scaled)\n    y_proba = lr.predict_proba(X_test_scaled)[:, 1]\n    \n    # Calculer et logger les métriques\n    metrics = {\n        \"accuracy\": accuracy_score(y_test, y_pred),\n        \"precision\": precision_score(y_test, y_pred),\n        \"recall\": recall_score(y_test, y_pred),\n        \"f1\": f1_score(y_test, y_pred)\n    }\n    mlflow.log_metrics(metrics)\n    \n    # Logger les résultats d'évaluation en CSV avec tempfile (pas de fichier local laissé)\n    eval_df = pd.DataFrame({\n        'actual': y_test.values,\n        'predicted': y_pred,\n        'probability': y_proba,\n        'correct': (y_test.values == y_pred).astype(int)\n    })\n    with tempfile.TemporaryDirectory() as tmpdir:\n        csv_path = os.path.join(tmpdir, \"evaluation_results.csv\")\n        eval_df.to_csv(csv_path, index=False)\n        mlflow.log_artifact(csv_path)\n    \n    # Logger le modèle (utiliser name= au lieu de artifact_path= déprécié)\n    mlflow.sklearn.log_model(lr, name=\"model\")\n    \n    # Logger la matrice de confusion - avec log_figure (pas de fichier local)\n    cm = confusion_matrix(y_test, y_pred)\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n    ax.set_xlabel('Prédit')\n    ax.set_ylabel('Réel')\n    ax.set_title('Matrice de Confusion - LogisticRegression')\n    plt.tight_layout()\n    mlflow.log_figure(fig, \"confusion_matrix.png\")\n    plt.close()\n    \n    lr_run_id = run.info.run_id\n    lr_metrics = metrics\n    \n    print(f\"Modèle : LogisticRegression | Accuracy : {metrics['accuracy']:.4f} | F1 : {metrics['f1']:.4f}\")\n    print(f\"Loggé : modèle, dataset, scaler, matrice de confusion, CSV d'évaluation\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Expérience 2 : Random Forest (paramètres par défaut)\nrf1 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\nrf1_run_id, rf1_metrics = train_and_log_model(\n    rf1,\n    \"RandomForest_v1\",\n    X_train, X_test, y_train, y_test,\n    params={\"n_estimators\": 100, \"max_depth\": None, \"min_samples_split\": 2}\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Expérience 3 : Random Forest (paramètres optimisés)\nrf2 = RandomForestClassifier(\n    n_estimators=200, \n    max_depth=10, \n    min_samples_split=20,\n    random_state=42, \n    n_jobs=-1\n)\nrf2_run_id, rf2_metrics = train_and_log_model(\n    rf2,\n    \"RandomForest_v2_tuned\",\n    X_train, X_test, y_train, y_test,\n    params={\"n_estimators\": 200, \"max_depth\": 10, \"min_samples_split\": 20}\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Expérience 4 : Gradient Boosting\ngb = GradientBoostingClassifier(\n    n_estimators=100, \n    learning_rate=0.1, \n    max_depth=5, \n    random_state=42\n)\ngb_run_id, gb_metrics = train_and_log_model(\n    gb,\n    \"GradientBoosting\",\n    X_train, X_test, y_train, y_test,\n    params={\"n_estimators\": 100, \"learning_rate\": 0.1, \"max_depth\": 5}\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Expérience 5 : Gradient Boosting (paramètres différents)\ngb2 = GradientBoostingClassifier(\n    n_estimators=200, \n    learning_rate=0.05, \n    max_depth=4, \n    random_state=42\n)\ngb2_run_id, gb2_metrics = train_and_log_model(\n    gb2,\n    \"GradientBoosting_v2\",\n    X_train, X_test, y_train, y_test,\n    params={\"n_estimators\": 200, \"learning_rate\": 0.05, \"max_depth\": 4}\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.3 Alternative : Autologging\n\nMLflow peut automatiquement logger les paramètres et métriques pour les frameworks supportés !\n\n**L'autolog capture :**\n- **Tous les paramètres** (y compris les défauts comme `bootstrap=True`, `ccp_alpha=0.0`, etc.)\n- **Métriques d'entraînement** avec des noms différents : `training_accuracy_score`, `training_f1_score`, etc.\n- **Artefact du modèle** et signature\n- **Info du dataset** (schéma d'entrée, lignes d'exemple)\n- **Importance des features** (pour les modèles basés sur les arbres)\n\n**Compromis :**\n| Logging Manuel | Autologging |\n|----------------|-------------|\n| Contrôle total sur ce qui est loggé | Logge tout automatiquement |\n| Noms de métriques cohérents (`accuracy`) | Noms spécifiques au framework (`training_accuracy_score`) |\n| Liste de paramètres propre | Tous les paramètres (y compris les défauts) |\n| Plus de code à écrire | Zéro code de logging nécessaire |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Activer l'autologging pour sklearn\nmlflow.sklearn.autolog()\n\n# Maintenant entraîner sans logging manuel - MLflow capture tout !\nwith mlflow.start_run(run_name=\"RandomForest_autolog\"):\n    mlflow.set_tag(\"logging_method\", \"autolog\")\n    \n    rf_auto = RandomForestClassifier(\n        n_estimators=150, \n        max_depth=12, \n        random_state=42,\n        n_jobs=-1\n    )\n    rf_auto.fit(X_train, y_train)\n    \n    # L'autolog capture automatiquement :\n    # - Tous les paramètres du modèle (y compris les défauts !)\n    # - Métriques d'entraînement (training_accuracy_score, etc.) - SUR LES DONNÉES D'ENTRAÎNEMENT\n    # - Artefact du modèle\n    # - Importance des features\n    \n    # IMPORTANT : Les métriques autolog sont sur les données d'ENTRAÎNEMENT !\n    # Pour une comparaison juste, on logge aussi les métriques de TEST manuellement :\n    y_pred = rf_auto.predict(X_test)\n    mlflow.log_metrics({\n        \"accuracy\": accuracy_score(y_test, y_pred),\n        \"precision\": precision_score(y_test, y_pred),\n        \"recall\": recall_score(y_test, y_pred),\n        \"f1\": f1_score(y_test, y_pred)\n    })\n    \n    print(\"L'autologging a capturé tous les paramètres et le modèle automatiquement !\")\n    print(f\"Note : les métriques training_* sont sur les données d'entraînement (seront plus élevées)\")\n    print(f\"      accuracy/f1/etc sont sur les données de test (comparables aux autres runs)\")\n\n# Désactiver l'autolog pour le contrôle manuel\nmlflow.sklearn.autolog(disable=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Comparer les expériences\n\nMaintenant voyons la puissance de MLflow - comparer toutes nos expériences !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Récupérer tous les runs de notre expérience\nclient = MlflowClient()\nexperiment = client.get_experiment_by_name(experiment_name)\n\nruns = client.search_runs(\n    experiment_ids=[experiment.experiment_id],\n    order_by=[\"metrics.f1 DESC\"]\n)\n\nprint(f\"Total des runs : {len(runs)}\\n\")\nprint(\"Toutes les expériences classées par score F1 (métriques de TEST) :\")\nprint(\"=\" * 80)\n\nresults = []\nfor run in runs:\n    if run.data.metrics:\n        metrics = run.data.metrics\n        \n        # Prioriser les métriques de TEST (accuracy, f1, etc.)\n        # Fallback sur les métriques d'entraînement seulement si les métriques de test n'existent pas\n        accuracy = metrics.get('accuracy', 0)\n        precision = metrics.get('precision', 0)\n        recall = metrics.get('recall', 0)\n        f1 = metrics.get('f1', 0)\n        \n        # Ignorer les runs qui n'ont pas de métriques de test\n        if f1 == 0 and 'training_f1_score' in metrics:\n            # Ce run n'a que des métriques d'entraînement - afficher mais signaler\n            accuracy = metrics.get('training_accuracy_score', 0)\n            f1 = metrics.get('training_f1_score', 0)\n            precision = metrics.get('training_precision_score', 0)\n            recall = metrics.get('training_recall_score', 0)\n            metric_type = \"TRAIN (non comparable !)\"\n        else:\n            metric_type = \"test\"\n        \n        logging_method = run.data.tags.get('logging_method', 'manual')\n        \n        results.append({\n            'run_name': run.info.run_name,\n            'run_id': run.info.run_id[:8],\n            'metric_type': metric_type,\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1': f1\n        })\n\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values('f1', ascending=False).reset_index(drop=True)\n\nprint(\"\\nNote : Tous les runs utilisent maintenant les métriques de TEST pour une comparaison juste.\")\nprint(\"      Les runs avec uniquement des métriques d'entraînement sont signalés.\\n\")\nresults_df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualiser la comparaison\nif len(results_df) > 0:\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    x = range(len(results_df))\n    width = 0.2\n    \n    ax.bar([i - 1.5*width for i in x], results_df['accuracy'], width, label='Accuracy')\n    ax.bar([i - 0.5*width for i in x], results_df['precision'], width, label='Precision')\n    ax.bar([i + 0.5*width for i in x], results_df['recall'], width, label='Recall')\n    ax.bar([i + 1.5*width for i in x], results_df['f1'], width, label='F1')\n    \n    ax.set_xlabel('Modèle')\n    ax.set_ylabel('Score')\n    ax.set_title('Comparaison des Modèles (depuis MLflow)')\n    ax.set_xticks(x)\n    ax.set_xticklabels(results_df['run_name'], rotation=45, ha='right')\n    ax.legend()\n    ax.set_ylim(0, 1)\n    \n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Trouver le meilleur modèle\nif len(results_df) > 0:\n    best_run = results_df.loc[results_df['f1'].idxmax()]\n    print(f\"Meilleur modèle par score F1 : {best_run['run_name']}\")\n    print(f\"Score F1 : {best_run['f1']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Model Registry\n\nMaintenant que nous avons trouvé notre meilleur modèle, **enregistrons-le**. Le Model Registry fournit :\n- Contrôle de version pour les modèles\n- Transitions d'état (Staging → Production)\n- Lignage du modèle (quel run a créé ce modèle)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Obtenir l'ID du meilleur run (par score F1)\nbest_runs = client.search_runs(\n    experiment_ids=[experiment.experiment_id],\n    filter_string=\"metrics.f1 > 0\",\n    order_by=[\"metrics.f1 DESC\"],\n    max_results=1\n)\n\nif best_runs:\n    best_run = best_runs[0]\n    best_run_id = best_run.info.run_id\n    print(f\"Meilleur run : {best_run.info.run_name}\")\n    print(f\"Run ID : {best_run_id}\")\n    print(f\"F1 : {best_run.data.metrics.get('f1', 0):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enregistrer le meilleur modèle\nmodel_name = \"customer-churn-classifier\"\n\nif best_runs:\n    model_uri = f\"runs:/{best_run_id}/model\"\n    \n    # Enregistrer le modèle\n    registered_model = mlflow.register_model(\n        model_uri=model_uri,\n        name=model_name\n    )\n    \n    print(f\"Modèle enregistré : {model_name}\")\n    print(f\"Version : {registered_model.version}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Voir les versions du modèle enregistré\ntry:\n    versions = client.search_model_versions(f\"name='{model_name}'\")\n    print(f\"\\nVersions enregistrées de '{model_name}' :\")\n    print(\"=\" * 60)\n    for v in versions:\n        print(f\"Version {v.version} : Run ID {v.run_id[:8]}... | Statut : {v.status}\")\nexcept Exception as e:\n    print(f\"Aucun modèle enregistré trouvé : {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Charger et utiliser le modèle enregistré\n\nEn production, vous chargez les modèles depuis le registry - pas depuis des fichiers locaux !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Charger le modèle depuis le registry avec les artefacts de prétraitement\nimport joblib\nimport tempfile\n\ntry:\n    # Charger la dernière version\n    model_uri = f\"models:/{model_name}/latest\"\n    loaded_model = mlflow.sklearn.load_model(model_uri)\n    \n    print(f\"Modèle chargé : {model_name}\")\n    print(f\"Type de modèle : {type(loaded_model).__name__}\")\n    \n    # Vérifier si ce modèle nécessite un prétraitement\n    client = MlflowClient()\n    model_version = client.get_latest_versions(model_name, stages=[])[0]\n    run_id = model_version.run_id\n    run = client.get_run(run_id)\n    \n    requires_scaling = run.data.tags.get(\"requires_scaling\", \"false\") == \"true\"\n    \n    if requires_scaling:\n        print(f\"\\n⚠️  Ce modèle nécessite un prétraitement (scaling)\")\n        \n        # Télécharger l'artefact du scaler\n        artifact_path = client.download_artifacts(run_id, \"preprocessing/scaler.pkl\")\n        scaler = joblib.load(artifact_path)\n        print(f\"✅ Scaler chargé depuis les artefacts\")\n        \n        # Appliquer le prétraitement avant la prédiction\n        X_test_scaled = scaler.transform(X_test[:5])\n        sample_predictions = loaded_model.predict(X_test_scaled)\n        sample_probas = loaded_model.predict_proba(X_test_scaled)[:, 1]\n        \n        print(\"\\nExemples de prédictions (avec prétraitement correct) :\")\n        for i, (pred, proba) in enumerate(zip(sample_predictions, sample_probas)):\n            actual = y_test.iloc[i]\n            print(f\"  Client {i+1} : Prédit={pred}, Réel={actual}, Probabilité={proba:.2%}\")\n    else:\n        # Les modèles basés sur les arbres n'ont pas besoin de scaling\n        print(f\"\\n✅ Ce modèle n'a pas besoin de prétraitement\")\n        sample_predictions = loaded_model.predict(X_test[:5])\n        sample_probas = loaded_model.predict_proba(X_test[:5])[:, 1]\n        \n        print(\"\\nExemples de prédictions :\")\n        for i, (pred, proba) in enumerate(zip(sample_predictions, sample_probas)):\n            actual = y_test.iloc[i]\n            print(f\"  Client {i+1} : Prédit={pred}, Réel={actual}, Probabilité={proba:.2%}\")\n        \nexcept Exception as e:\n    print(f\"Impossible de charger depuis le registry : {e}\")\n    print(\"C'est attendu si c'est la première exécution.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Point clé à retenir\n\nRemarquez comment le code ci-dessus :\n1. **Vérifie les tags du modèle** pour voir si un prétraitement est nécessaire\n2. **Télécharge l'artefact du scaler** depuis MLflow\n3. **Applique la même transformation** utilisée pendant l'entraînement\n\nC'est le **pattern prêt pour la production** :\n```python\n# Pendant l'entraînement\nscaler.fit_transform(X_train)  # Apprendre des données d'entraînement\nmlflow.log_artifact(\"scaler.pkl\")  # Le sauvegarder !\n\n# Pendant l'inférence  \nscaler = load_artifact(\"scaler.pkl\")  # Le charger !\nscaler.transform(X_new)  # Appliquer (ne jamais refaire fit !)\n```\n\n**Erreur courante :** Refaire fit du scaler sur de nouvelles données → échelle différente → prédictions incorrectes !"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Voir l'UI MLflow\n\nLe serveur MLflow tourne dans Docker. Ouvrez http://localhost:5000 dans votre navigateur.\n\nVous verrez :\n- Toutes les expériences et runs\n- Comparaison de paramètres et métriques\n- Navigateur d'artefacts (modèles, graphiques)\n- Model registry\n\n**Note :** Si vous n'avez pas encore démarré le serveur, exécutez :\n```bash\ncd docker && docker-compose up -d\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Résumé : Ce que MLflow a résolu\n\n| Problème (Notebook chaotique) | Solution (MLflow) |\n|-------------------------------|-------------------|\n| \"C'était lequel le meilleur modèle ?\" | Requêter les runs par métriques |\n| Métriques éparpillées dans les cellules | Stockage centralisé des métriques |\n| Pas de versioning | Model Registry avec versions |\n| Impossible de reproduire | Run ID lié au code/params exact |\n| Comparaison manuelle | UI de comparaison automatique |\n| Fichiers de modèles perdus | Artefacts stockés avec le run |\n\n## Ce qui reste manuel ?\n\nMLflow ne résout pas tout :\n- **Planification** : Comment exécuter ça quotidiennement ?\n- **Dépendances** : Et si le chargement des données échoue ?\n- **Alertes** : Comment savoir si l'entraînement a échoué ?\n- **CI/CD** : Comment automatiser le déploiement ?\n\n**Prochaine étape** : Orchestrateurs (Prefect, Airflow, Dagster) pour automatiser le pipeline !\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercices\n\n1. **Ajouter un nouveau modèle** : Essayez XGBoost ou LightGBM et loggez-le dans MLflow\n2. **Recherche d'hyperparamètres** : Lancez plusieurs expériences avec différents paramètres\n3. **Métriques personnalisées** : Loggez une métrique personnalisée (ex : AUC-ROC)\n4. **Comparaison de modèles** : Utilisez l'UI MLflow pour comparer vos expériences\n5. **Workflow du registry** : Enregistrez votre meilleur modèle et rechargez-le"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Atelier MlFlow_orchestrateur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}