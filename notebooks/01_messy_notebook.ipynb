{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Prédiction du Churn Client\n## Notebook d'exploration - En cours\n\nTODO: nettoyer tout ça plus tard"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Charger les données clients\n# (exécuter `python generate_sample_data.py` d'abord si le fichier n'existe pas)\ndf = pd.read_csv('../data/customer_data.csv')\n\nprint(f\"Dimensions du dataset: {df.shape}\")\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Analyse Exploratoire (EDA)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   customer_id        5000 non-null   int64  \n",
      " 1   recency_days       5000 non-null   int64  \n",
      " 2   frequency          5000 non-null   int64  \n",
      " 3   monetary_value     5000 non-null   float64\n",
      " 4   avg_order_value    5000 non-null   float64\n",
      " 5   days_since_signup  5000 non-null   int64  \n",
      " 6   total_orders       5000 non-null   int64  \n",
      " 7   support_tickets    5000 non-null   int64  \n",
      " 8   age                5000 non-null   int64  \n",
      " 9   churned            5000 non-null   int64  \n",
      "dtypes: float64(2), int64(8)\n",
      "memory usage: 390.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "customer_id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recency_days",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "frequency",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "monetary_value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_order_value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "days_since_signup",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_orders",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "support_tickets",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "age",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "churned",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ee8acc3b-b99f-4d6c-8939-6ec86722772a",
       "rows": [
        [
         "count",
         "5000.0",
         "5000.0",
         "5000.0",
         "5000.0",
         "5000.0",
         "5000.0",
         "5000.0",
         "5000.0",
         "5000.0",
         "5000.0"
        ],
        [
         "mean",
         "2500.5",
         "69.0588",
         "10.7284",
         "1573.179302",
         "244.907142",
         "500.763",
         "7.5624",
         "0.9722",
         "39.9954",
         "0.4904"
        ],
        [
         "std",
         "1443.5200033252052",
         "67.94108728029133",
         "5.180810901810649",
         "1240.7204733440035",
         "227.64604748477169",
         "304.7460202502597",
         "5.89473119074672",
         "1.3291277124848533",
         "13.784679260558198",
         "0.4999578297879866"
        ],
        [
         "min",
         "1.0",
         "0.0",
         "0.0",
         "50.0",
         "10.0",
         "30.0",
         "1.0",
         "0.0",
         "18.0",
         "0.0"
        ],
        [
         "25%",
         "1250.75",
         "19.0",
         "7.0",
         "707.9849999999999",
         "94.705",
         "274.0",
         "3.0",
         "0.0",
         "30.0",
         "0.0"
        ],
        [
         "50%",
         "2500.5",
         "48.0",
         "10.0",
         "1272.0549999999998",
         "174.84",
         "438.0",
         "6.0",
         "1.0",
         "40.0",
         "0.0"
        ],
        [
         "75%",
         "3750.25",
         "98.0",
         "14.0",
         "2077.46",
         "315.13",
         "664.25",
         "10.0",
         "1.0",
         "49.0",
         "1.0"
        ],
        [
         "max",
         "5000.0",
         "365.0",
         "31.0",
         "11958.63",
         "3034.51",
         "1500.0",
         "44.0",
         "11.0",
         "75.0",
         "1.0"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>recency_days</th>\n",
       "      <th>frequency</th>\n",
       "      <th>monetary_value</th>\n",
       "      <th>avg_order_value</th>\n",
       "      <th>days_since_signup</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>support_tickets</th>\n",
       "      <th>age</th>\n",
       "      <th>churned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2500.500000</td>\n",
       "      <td>69.058800</td>\n",
       "      <td>10.728400</td>\n",
       "      <td>1573.179302</td>\n",
       "      <td>244.907142</td>\n",
       "      <td>500.76300</td>\n",
       "      <td>7.562400</td>\n",
       "      <td>0.972200</td>\n",
       "      <td>39.995400</td>\n",
       "      <td>0.490400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1443.520003</td>\n",
       "      <td>67.941087</td>\n",
       "      <td>5.180811</td>\n",
       "      <td>1240.720473</td>\n",
       "      <td>227.646047</td>\n",
       "      <td>304.74602</td>\n",
       "      <td>5.894731</td>\n",
       "      <td>1.329128</td>\n",
       "      <td>13.784679</td>\n",
       "      <td>0.499958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1250.750000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>707.985000</td>\n",
       "      <td>94.705000</td>\n",
       "      <td>274.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2500.500000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1272.055000</td>\n",
       "      <td>174.840000</td>\n",
       "      <td>438.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3750.250000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2077.460000</td>\n",
       "      <td>315.130000</td>\n",
       "      <td>664.25000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>11958.630000</td>\n",
       "      <td>3034.510000</td>\n",
       "      <td>1500.00000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       customer_id  recency_days    frequency  monetary_value  \\\n",
       "count  5000.000000   5000.000000  5000.000000     5000.000000   \n",
       "mean   2500.500000     69.058800    10.728400     1573.179302   \n",
       "std    1443.520003     67.941087     5.180811     1240.720473   \n",
       "min       1.000000      0.000000     0.000000       50.000000   \n",
       "25%    1250.750000     19.000000     7.000000      707.985000   \n",
       "50%    2500.500000     48.000000    10.000000     1272.055000   \n",
       "75%    3750.250000     98.000000    14.000000     2077.460000   \n",
       "max    5000.000000    365.000000    31.000000    11958.630000   \n",
       "\n",
       "       avg_order_value  days_since_signup  total_orders  support_tickets  \\\n",
       "count      5000.000000         5000.00000   5000.000000      5000.000000   \n",
       "mean        244.907142          500.76300      7.562400         0.972200   \n",
       "std         227.646047          304.74602      5.894731         1.329128   \n",
       "min          10.000000           30.00000      1.000000         0.000000   \n",
       "25%          94.705000          274.00000      3.000000         0.000000   \n",
       "50%         174.840000          438.00000      6.000000         1.000000   \n",
       "75%         315.130000          664.25000     10.000000         1.000000   \n",
       "max        3034.510000         1500.00000     44.000000        11.000000   \n",
       "\n",
       "               age      churned  \n",
       "count  5000.000000  5000.000000  \n",
       "mean     39.995400     0.490400  \n",
       "std      13.784679     0.499958  \n",
       "min      18.000000     0.000000  \n",
       "25%      30.000000     0.000000  \n",
       "50%      40.000000     0.000000  \n",
       "75%      49.000000     1.000000  \n",
       "max      75.000000     1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# vérifier la distribution de la cible\ndf['churned'].value_counts()"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "churned",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "proportion",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "27e578b6-ba7f-4824-be2f-8987f7a3c69e",
       "rows": [
        [
         "0",
         "0.5096"
        ],
        [
         "1",
         "0.4904"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "churned\n",
       "0    0.5096\n",
       "1    0.4904\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['churned'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# vérifier les valeurs manquantes\ndf.isnull().sum()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# graphiques de distribution\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\naxes = axes.flatten()\n\nfor i, col in enumerate(df.columns[1:-1]):  # ignorer customer_id et churned\n    axes[i].hist(df[col], bins=30, edgecolor='black')\n    axes[i].set_title(col)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# boxplots par statut de churn\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\naxes = axes.flatten()\n\nfor i, col in enumerate(df.columns[1:-1]):\n    df.boxplot(column=col, by='churned', ax=axes[i])\n    axes[i].set_title(col)\n\nplt.suptitle('')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# matrice de corrélation\nplt.figure(figsize=(10, 8))\nsns.heatmap(df.drop('customer_id', axis=1).corr(), annot=True, cmap='coolwarm', center=0)\nplt.title('Matrice de Corrélation')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Feature Engineering\n\nCréons quelques nouvelles features"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# créer de nouvelles features\ndf['recency_frequency_ratio'] = df['recency_days'] / (df['frequency'] + 1)\ndf['monetary_per_order'] = df['monetary_value'] / (df['total_orders'] + 1)\ndf['order_frequency'] = df['total_orders'] / (df['days_since_signup'] + 1)\ndf['support_per_order'] = df['support_tickets'] / (df['total_orders'] + 1)\n\n# Score RFM - binning manuel\ndf['r_score'] = pd.qcut(df['recency_days'], q=5, labels=[5, 4, 3, 2, 1]).astype(int)\ndf['f_score'] = pd.qcut(df['frequency'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5]).astype(int)\ndf['m_score'] = pd.qcut(df['monetary_value'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5]).astype(int)\ndf['rfm_score'] = df['r_score'] + df['f_score'] + df['m_score']\n\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 18)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Préparation des données pour la modélisation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# features à utiliser\nfeature_cols = [\n    'recency_days', 'frequency', 'monetary_value', 'avg_order_value',\n    'days_since_signup', 'total_orders', 'support_tickets', 'age',\n    'recency_frequency_ratio', 'monetary_per_order', 'order_frequency',\n    'support_per_order', 'rfm_score'\n]\n\nX = df[feature_cols]\ny = df['churned']\n\nprint(f\"Dimensions des features: {X.shape}\")\nprint(f\"Dimensions de la cible: {y.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# séparation train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"Ensemble d'entraînement: {X_train.shape[0]} échantillons\")\nprint(f\"Ensemble de test: {X_test.shape[0]} échantillons\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# normalisation des features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Modèle 1: Régression Logistique"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# régression logistique\nlr = LogisticRegression(random_state=42, max_iter=1000)\nlr.fit(X_train_scaled, y_train)\n\ny_pred_lr = lr.predict(X_test_scaled)\n\nprint(\"Résultats Régression Logistique:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_lr):.4f}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_lr):.4f}\")\nprint(f\"F1: {f1_score(y_test, y_pred_lr):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# validation croisée\ncv_scores = cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='accuracy')\nprint(f\"CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Modèle 2: Random Forest"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# random forest - essayer différents paramètres\nrf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\nrf.fit(X_train, y_train)\n\ny_pred_rf = rf.predict(X_test)\n\nprint(\"Random Forest (n_estimators=100) Résultats:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_rf):.4f}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_rf):.4f}\")\nprint(f\"F1: {f1_score(y_test, y_pred_rf):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# hmm essayons avec plus d'arbres\nrf2 = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\nrf2.fit(X_train, y_train)\n\ny_pred_rf2 = rf2.predict(X_test)\n\nprint(\"Random Forest (n_estimators=200, max_depth=10) Résultats:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_rf2):.4f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_rf2):.4f}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_rf2):.4f}\")\nprint(f\"F1: {f1_score(y_test, y_pred_rf2):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# essayons une autre configuration\nrf3 = RandomForestClassifier(n_estimators=150, max_depth=15, min_samples_split=10, random_state=42, n_jobs=-1)\nrf3.fit(X_train, y_train)\n\ny_pred_rf3 = rf3.predict(X_test)\n\nprint(\"Random Forest (n_estimators=150, max_depth=15, min_samples_split=10) Résultats:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_rf3):.4f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_rf3):.4f}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_rf3):.4f}\")\nprint(f\"F1: {f1_score(y_test, y_pred_rf3):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# validation croisée sur le meilleur RF jusqu'ici\ncv_scores_rf = cross_val_score(rf2, X_train, y_train, cv=5, scoring='accuracy')\nprint(f\"RF CV Accuracy: {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std()*2:.4f})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# importance des features\nfeat_imp = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': rf2.feature_importances_\n}).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nplt.barh(feat_imp['feature'], feat_imp['importance'])\nplt.xlabel('Importance')\nplt.title('Importance des Features (Random Forest)')\nplt.gca().invert_yaxis()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Modèle 3: Gradient Boosting"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# gradient boosting\ngb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\ngb.fit(X_train, y_train)\n\ny_pred_gb = gb.predict(X_test)\n\nprint(\"Résultats Gradient Boosting:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_gb):.4f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_gb):.4f}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_gb):.4f}\")\nprint(f\"F1: {f1_score(y_test, y_pred_gb):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# essayons un learning rate différent\ngb2 = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42)\ngb2.fit(X_train, y_train)\n\ny_pred_gb2 = gb2.predict(X_test)\n\nprint(\"Gradient Boosting (lr=0.05, n=200) Résultats:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_gb2):.4f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_gb2):.4f}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_gb2):.4f}\")\nprint(f\"F1: {f1_score(y_test, y_pred_gb2):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Optimisation des hyperparamètres avec GridSearch\n\nEssayons une grid search sur RF"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# grid search - ça prend du temps\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [5, 10, 15],\n    'min_samples_split': [2, 10, 20]\n}\n\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42, n_jobs=-1),\n    param_grid,\n    cv=5,\n    scoring='f1',\n    verbose=1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Meilleurs paramètres: {grid_search.best_params_}\")\nprint(f\"Meilleur score: {grid_search.best_score_:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# évaluer le meilleur modèle\nbest_rf = grid_search.best_estimator_\ny_pred_best = best_rf.predict(X_test)\n\nprint(\"Résultats Meilleur Random Forest:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_best):.4f}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_best):.4f}\")\nprint(f\"F1: {f1_score(y_test, y_pred_best):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# matrice de confusion\ncm = confusion_matrix(y_test, y_pred_best)\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Prédit')\nplt.ylabel('Réel')\nplt.title('Matrice de Confusion')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.79       510\n",
      "           1       0.78      0.78      0.78       490\n",
      "\n",
      "    accuracy                           0.78      1000\n",
      "   macro avg       0.78      0.78      0.78      1000\n",
      "weighted avg       0.78      0.78      0.78      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Résumé\n\n| Modèle | Accuracy | Precision | Recall | F1 |\n|--------|----------|-----------|--------|----|\n| Régression Logistique | ? | ? | ? | ? |\n| Random Forest (100) | ? | ? | ? | ? |\n| Random Forest (200, depth=10) | ? | ? | ? | ? |\n| Gradient Boosting | ? | ? | ? | ? |\n| Meilleur RF (grid search) | ? | ? | ? | ? |\n\nTODO: remplir le tableau manuellement...\n\nLe meilleur modèle semble être RF avec les paramètres de grid search. Il faut sauvegarder ce modèle et le déployer quelque part."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# sauvegarder le modèle? je sais pas où\n# import pickle\n# with open('model.pkl', 'wb') as f:\n#     pickle.dump(best_rf, f)\n\n# attends c'était lequel le meilleur déjà? laisse moi remonter..."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# en fait laisse moi essayer encore un truc - et si je retirais des features?\n# peut-être que je suis en overfitting\n\n# garder seulement les top features\ntop_features = feat_imp.head(8)['feature'].tolist()\nprint(f\"Top features: {top_features}\")\n\nX_train_top = X_train[top_features]\nX_test_top = X_test[top_features]\n\nrf_top = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\nrf_top.fit(X_train_top, y_train)\n\ny_pred_top = rf_top.predict(X_test_top)\n\nprint(\"\\nRF avec top 8 features:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_top):.4f}\")\nprint(f\"F1: {f1_score(y_test, y_pred_top):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ok j'abandonne d'essayer de suivre toutes ces expériences\n# c'était lequel le meilleur? je crois que c'était celui de grid search?\n# ou alors gb2?\n\n# TODO: utiliser mlflow ou autre chose pour tracker tout ça..."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Notes pour moi-même\n\n- Meilleur modèle: probablement RF de grid search?\n- Besoin de trouver comment mieux tracker les expériences\n- Est-ce que je dois sauvegarder le scaler aussi?\n- Comment je déploie ça?\n- Marie a dit quelque chose sur MLflow?\n- Il faut aussi exécuter ça tous les jours pour les nouveaux clients - manuellement? cron job?\n\n**Problèmes avec ce notebook:**\n1. Je ne me souviens plus quel modèle était le meilleur\n2. Les métriques sont éparpillées partout\n3. Pas de versioning des modèles\n4. Impossible de reproduire exactement les résultats\n5. Pas d'automatisation\n6. Tout est hardcodé"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Atelier MlFlow_orchestrateur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}