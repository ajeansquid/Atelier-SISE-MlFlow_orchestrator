{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Du Chaos à MLflow : Un Parcours Guidé\n\n## Objectif du Workshop\n\nVous venez d'expérimenter le **notebook chaotique** (`01_messy_notebook.ipynb`). Vous avez probablement remarqué :\n\n- \"Attends, c'était lequel le meilleur modèle déjà?\"\n- Métriques éparpillées dans les cellules, difficiles à comparer\n- Aucun moyen de reproduire exactement les résultats\n- Tracking manuel dans des tableaux markdown (que vous avez oublié de mettre à jour)\n\n**Dans ce notebook, nous allons corriger tout ça avec MLflow.**\n\n---\n\n## Comment fonctionne ce notebook\n\nNous allons progressivement augmenter la difficulté :\n\n| Partie | Style | Vous allez... |\n|--------|-------|---------------|\n| **Partie 1** | Entièrement guidé | Exécuter le code, observer, apprendre les concepts |\n| **Partie 2** | Révélation progressive | Deviner ce qui est nécessaire, puis voir la solution |\n| **Partie 3** | Compléter les blancs | Compléter le code vous-même |\n\n**Référence** : Voir `02_mlflow_organized.ipynb` pour la solution complète.\n\n**Cheatsheet** : Voir `../docs/mlflow_cheatsheet.md` pour une référence rapide.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prérequis\n\nAvant de commencer, assurez-vous que :\n1. Le serveur MLflow est en cours d'exécution : `docker-compose up -d` (depuis la racine du projet)\n2. Vérifiez que http://localhost:5000 est accessible"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup : Charger les données du notebook chaotique\n\nD'abord, reproduisons la même préparation de données que dans le notebook chaotique."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports standards (identiques au notebook chaotique)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Imports terminés!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Charger et préparer les données (identique au notebook chaotique)\ndf = pd.read_csv('../data/customer_data.csv')\n\n# Feature engineering\ndf['recency_frequency_ratio'] = df['recency_days'] / (df['frequency'] + 1)\ndf['monetary_per_order'] = df['monetary_value'] / (df['total_orders'] + 1)\ndf['order_frequency'] = df['total_orders'] / (df['days_since_signup'] + 1)\ndf['support_per_order'] = df['support_tickets'] / (df['total_orders'] + 1)\ndf['r_score'] = pd.qcut(df['recency_days'], q=5, labels=[5, 4, 3, 2, 1]).astype(int)\ndf['f_score'] = pd.qcut(df['frequency'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5]).astype(int)\ndf['m_score'] = pd.qcut(df['monetary_value'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5]).astype(int)\ndf['rfm_score'] = df['r_score'] + df['f_score'] + df['m_score']\n\n# Définir les features\nFEATURE_COLS = [\n    'recency_days', 'frequency', 'monetary_value', 'avg_order_value',\n    'days_since_signup', 'total_orders', 'support_tickets', 'age',\n    'recency_frequency_ratio', 'monetary_per_order', 'order_frequency',\n    'support_per_order', 'rfm_score'\n]\n\nX = df[FEATURE_COLS]\ny = df['churned']\n\n# Séparation train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Données prêtes : {len(X_train)} train, {len(X_test)} test échantillons\")\nprint(f\"Features : {len(FEATURE_COLS)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# PARTIE 1 : Entièrement Guidé\n\n## 1.1 Qu'est-ce que MLflow ?\n\nMLflow est une plateforme open-source pour gérer le **cycle de vie complet du ML**. Elle a 4 composants principaux :\n\n### 1. **Tracking** (notre focus principal aujourd'hui)\nEnregistrer et requêter les expériences : paramètres, métriques, artefacts et versions du code.\n- *\"Quels hyperparamètres j'ai utilisé pour mon meilleur modèle ?\"*\n- *\"Comparer l'accuracy sur 50 runs\"*\n\n### 2. **Models**\nPackager les modèles ML dans un format standard pour le déploiement sur diverses plateformes (Docker, Kubernetes, services cloud, etc.).\n- *\"Déployer ce modèle sklearn comme API REST\"*\n- *\"Convertir mon modèle pour de l'inférence batch\"*\n\n### 3. **Model Registry** (on va l'utiliser aussi)\nStore centralisé de modèles avec versioning, transitions d'état (Staging → Production), et annotations.\n- *\"Promouvoir le modèle v3 en production\"*\n- *\"Qui a approuvé cette version du modèle ?\"*\n\n### 4. **Projects**\nPackager le code et les dépendances pour des exécutions reproductibles sur n'importe quelle plateforme.\n- *\"Exécuter cette expérience avec exactement le même environnement\"*\n- *\"Partager mon pipeline avec l'équipe\"*\n\n---\n\n### MLflow pour les LLMs (Bonus)\n\nMLflow supporte maintenant les **applications LLM/GenAI** :\n- **Tracing** : Débugger et monitorer les appels LLM, pipelines RAG, et agents multi-étapes\n- **Évaluation** : Benchmarker les outputs LLM avec des métriques intégrées\n- **Déploiement** : Servir les modèles LLM avec la même infrastructure de serving\n\n*On ne couvre pas le tracking LLM aujourd'hui, mais sachez que MLflow fonctionne au-delà du ML traditionnel !*\n\n---\n\n**Aujourd'hui on se concentre sur Tracking et Registry** - les fondations pour la gestion d'expériences."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 Se connecter à MLflow\n\nD'abord, on importe MLflow et on lui indique où se trouve notre serveur de tracking."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# NOUVEAU : Importer MLflow\nimport mlflow\nimport mlflow.sklearn\n\nprint(f\"Version MLflow : {mlflow.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Se connecter au serveur MLflow (qui tourne dans Docker)\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n\n# Vérifier la connexion\nimport requests\ntry:\n    response = requests.get(\"http://localhost:5000/health\")\n    if response.status_code == 200:\n        print(\"Le serveur MLflow fonctionne !\")\n    else:\n        print(f\"Le serveur a répondu avec : {response.status_code}\")\nexcept Exception as e:\n    print(f\"Impossible de se connecter au serveur MLflow : {e}\")\n    print(\"Exécutez 'docker-compose up -d' depuis la racine du projet !\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.3 Créer une Expérience\n\nUne **Expérience** regroupe les runs associés ensemble. Pensez-y comme un dossier de projet."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Créer ou récupérer une expérience\nexperiment_name = \"workshop-churn-learning\"\nmlflow.set_experiment(experiment_name)\n\nprint(f\"Utilisation de l'expérience : {experiment_name}\")\nprint(f\"Voir sur : http://localhost:5000\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.4 Votre Premier Run MLflow\n\nUn **Run** est une exécution unique de votre code d'entraînement. Dans un run, vous pouvez logger :\n\n- **Parameters** : Les entrées (hyperparamètres, config)\n- **Metrics** : Les sorties (accuracy, loss, F1)\n- **Artifacts** : Les fichiers (modèles, graphiques, données)\n- **Tags** : Les métadonnées (auteur, version)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Entraînons un modèle simple et LOGGONS TOUT\n\n# Démarrer un run\nwith mlflow.start_run(run_name=\"mon-premier-run-mlflow\"):\n    \n    # 1. LOGGER LES PARAMÈTRES (entrées de votre modèle)\n    mlflow.log_param(\"model_type\", \"RandomForest\")\n    mlflow.log_param(\"n_estimators\", 100)\n    mlflow.log_param(\"max_depth\", 10)\n    \n    # 2. Entraîner le modèle (même code qu'avant)\n    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n    model.fit(X_train, y_train)\n    \n    # 3. Faire des prédictions\n    y_pred = model.predict(X_test)\n    \n    # 4. LOGGER LES MÉTRIQUES (sorties/résultats)\n    accuracy = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    \n    mlflow.log_metric(\"accuracy\", accuracy)\n    mlflow.log_metric(\"f1\", f1)\n    \n    print(f\"Accuracy : {accuracy:.4f}\")\n    print(f\"Score F1 : {f1:.4f}\")\n    print(\"\\nRun enregistré dans MLflow !\")\n    print(\"Allez sur http://localhost:5000 pour le voir !\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Allez voir l'UI MLflow !\n\n1. Ouvrez http://localhost:5000\n2. Cliquez sur l'expérience \"workshop-churn-learning\"\n3. Cliquez sur \"mon-premier-run-mlflow\"\n4. Voyez vos paramètres et métriques !\n\n**C'est déjà mieux que le notebook chaotique** - vos résultats sont sauvegardés et organisés !"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.5 Logger plusieurs métriques à la fois\n\nAu lieu d'appeler `log_metric` plusieurs fois, vous pouvez logger un dictionnaire."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plus efficace : logger plusieurs métriques d'un coup\nwith mlflow.start_run(run_name=\"exemple-metriques-multiples\"):\n    \n    # Logger les params en dict\n    mlflow.log_params({\n        \"model_type\": \"RandomForest\",\n        \"n_estimators\": 150,\n        \"max_depth\": 12\n    })\n    \n    # Entraîner\n    model = RandomForestClassifier(n_estimators=150, max_depth=12, random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    # Logger les métriques en dict\n    mlflow.log_metrics({\n        \"accuracy\": accuracy_score(y_test, y_pred),\n        \"precision\": precision_score(y_test, y_pred),\n        \"recall\": recall_score(y_test, y_pred),\n        \"f1\": f1_score(y_test, y_pred)\n    })\n    \n    print(\"Toutes les métriques loggées !\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# PARTIE 2 : Révélation Progressive\n\nMaintenant vous comprenez les bases. Ajoutons plus de fonctionnalités MLflow.\n\n**Format** : Je vous montre un problème, vous réfléchissez à ce qui est nécessaire, puis je révèle la solution."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.1 Problème : Sauvegarder le modèle\n\nDans le notebook chaotique, on avait :\n```python\n# sauvegarder le modèle? je sais pas où\n# import pickle\n# with open('model.pkl', 'wb') as f:\n#     pickle.dump(best_rf, f)\n```\n\n**Question** : Comment peut-on sauvegarder le modèle AVEC le run, pour qu'il soit toujours lié à ses métriques ?\n\n*Réfléchissez-y... puis exécutez la cellule suivante pour voir la solution.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SOLUTION : Utiliser mlflow.sklearn.log_model()\n\nwith mlflow.start_run(run_name=\"avec-artefact-modele\"):\n    \n    mlflow.log_params({\"n_estimators\": 100, \"max_depth\": 10})\n    \n    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    mlflow.log_metrics({\n        \"accuracy\": accuracy_score(y_test, y_pred),\n        \"f1\": f1_score(y_test, y_pred)\n    })\n    \n    # LOGGER LE MODÈLE - il est maintenant sauvegardé avec ce run !\n    mlflow.sklearn.log_model(model, name=\"model\")\n    \n    print(\"Modèle sauvegardé comme artefact !\")\n    print(\"Vérifiez l'onglet 'Artifacts' dans l'UI MLflow\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.2 Problème : Sauvegarder les graphiques\n\nDans le notebook chaotique, on créait des matrices de confusion et des graphiques d'importance des features.\nMais ils s'affichaient juste dans le notebook - pas sauvegardés nulle part !\n\n**Question** : Comment peut-on sauvegarder une figure matplotlib comme artefact ?\n\n*Réfléchissez... puis révélez.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SOLUTION : Utiliser mlflow.log_figure()\n\nwith mlflow.start_run(run_name=\"avec-graphiques\"):\n    \n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    mlflow.log_metrics({\"accuracy\": accuracy_score(y_test, y_pred)})\n    \n    # Créer le graphique de matrice de confusion\n    cm = confusion_matrix(y_test, y_pred)\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n    ax.set_xlabel('Prédit')\n    ax.set_ylabel('Réel')\n    ax.set_title('Matrice de Confusion')\n    \n    # LOGGER LA FIGURE - pas de fichier local créé !\n    mlflow.log_figure(fig, \"matrice_confusion.png\")\n    plt.close()\n    \n    print(\"Graphique sauvegardé comme artefact !\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.3 Problème : Le problème du Scaler\n\nPour la Régression Logistique, on doit normaliser les features :\n```python\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n```\n\n**Problème** : Au moment de l'inférence, on a besoin du MÊME scaler. Si on ne le sauvegarde pas, on ne peut pas faire de prédictions correctes plus tard !\n\n**Question** : Comment sauvegarder le scaler avec le modèle ?\n\n*Réfléchissez... puis révélez.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SOLUTION : Sauvegarder le scaler comme artefact\nimport joblib\nimport tempfile\nimport os\n\nwith mlflow.start_run(run_name=\"logistique-avec-scaler\"):\n    \n    # Normaliser les données\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Sauvegarder le scaler comme artefact (avec tempfile pour éviter les fichiers locaux)\n    with tempfile.TemporaryDirectory() as tmpdir:\n        scaler_path = os.path.join(tmpdir, \"scaler.pkl\")\n        joblib.dump(scaler, scaler_path)\n        mlflow.log_artifact(scaler_path, artifact_path=\"preprocessing\")\n    \n    # Entraîner le modèle\n    model = LogisticRegression(random_state=42, max_iter=1000)\n    model.fit(X_train_scaled, y_train)\n    y_pred = model.predict(X_test_scaled)\n    \n    mlflow.log_metrics({\"accuracy\": accuracy_score(y_test, y_pred)})\n    mlflow.sklearn.log_model(model, name=\"model\")\n    \n    # Tag pour se rappeler que ce modèle nécessite un scaling\n    mlflow.set_tag(\"requires_scaling\", \"true\")\n    \n    print(\"Modèle + scaler sauvegardés !\")\n    print(\"Vérifiez les artefacts : preprocessing/scaler.pkl\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.4 Problème : Comparer les runs\n\nDans le notebook chaotique :\n```python\n# ok j'abandonne d'essayer de suivre toutes ces expériences\n# c'était lequel le meilleur? je crois que c'était celui de grid search?\n```\n\n**Question** : Comment trouver programmatiquement le meilleur run ?\n\n*Réfléchissez... puis révélez.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SOLUTION : Utiliser le Client MLflow pour rechercher les runs\nfrom mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\nexperiment = client.get_experiment_by_name(experiment_name)\n\n# Rechercher les runs, triés par accuracy (décroissant)\nruns = client.search_runs(\n    experiment_ids=[experiment.experiment_id],\n    order_by=[\"metrics.accuracy DESC\"],\n    max_results=5\n)\n\nprint(\"Top 5 des runs par accuracy :\")\nprint(\"=\" * 60)\nfor run in runs:\n    acc = run.data.metrics.get('accuracy', 0)\n    f1 = run.data.metrics.get('f1', 0)\n    print(f\"{run.info.run_name}: accuracy={acc:.4f}, f1={f1:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# PARTIE 3 : Compléter les blancs\n\nMaintenant c'est à vous ! Complétez le code dans les cellules suivantes.\n\n**Aide** : Référez-vous au cheatsheet dans `../docs/mlflow_cheatsheet.md`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.1 Exercice : Logger un modèle Gradient Boosting\n\nComplétez le code pour entraîner et logger un modèle Gradient Boosting avec :\n- Paramètres : n_estimators, learning_rate, max_depth\n- Métriques : accuracy, precision, recall, f1\n- Artefact : le modèle entraîné"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# EXERCICE : Complétez ce code\n\nwith mlflow.start_run(run_name=\"exercice-gradient-boosting\"):\n    \n    # TODO : Logger ces paramètres\n    n_estimators = 100\n    learning_rate = 0.1\n    max_depth = 5\n    \n    # mlflow.log_params({...})  # <-- Complétez ceci\n    \n    # Entraîner le modèle\n    model = GradientBoostingClassifier(\n        n_estimators=n_estimators,\n        learning_rate=learning_rate,\n        max_depth=max_depth,\n        random_state=42\n    )\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    # TODO : Calculer et logger les métriques\n    # mlflow.log_metrics({...})  # <-- Complétez ceci\n    \n    # TODO : Logger le modèle\n    # mlflow.sklearn.log_model(...)  # <-- Complétez ceci\n    \n    print(\"Exercice terminé ! Vérifiez l'UI MLflow.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Cliquez pour révéler la solution</summary>\n\n```python\nwith mlflow.start_run(run_name=\"exercice-gradient-boosting\"):\n    \n    n_estimators = 100\n    learning_rate = 0.1\n    max_depth = 5\n    \n    mlflow.log_params({\n        \"n_estimators\": n_estimators,\n        \"learning_rate\": learning_rate,\n        \"max_depth\": max_depth\n    })\n    \n    model = GradientBoostingClassifier(\n        n_estimators=n_estimators,\n        learning_rate=learning_rate,\n        max_depth=max_depth,\n        random_state=42\n    )\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    mlflow.log_metrics({\n        \"accuracy\": accuracy_score(y_test, y_pred),\n        \"precision\": precision_score(y_test, y_pred),\n        \"recall\": recall_score(y_test, y_pred),\n        \"f1\": f1_score(y_test, y_pred)\n    })\n    \n    mlflow.sklearn.log_model(model, name=\"model\")\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.2 Exercice : Ajouter des tags et une matrice de confusion\n\nAméliorez votre run avec :\n- Tags : author (votre nom), model_type\n- Un graphique de matrice de confusion comme artefact"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# EXERCICE : Complétez ce code\n\nwith mlflow.start_run(run_name=\"exercice-avec-tags-et-graphique\"):\n    \n    # TODO : Ajouter des tags\n    # mlflow.set_tag(\"author\", \"...\")  # <-- Votre nom\n    # mlflow.set_tag(\"model_type\", \"...\")  # <-- Type de modèle\n    \n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))\n    \n    # TODO : Créer et logger la matrice de confusion\n    # cm = confusion_matrix(y_test, y_pred)\n    # fig, ax = plt.subplots(...)\n    # sns.heatmap(...)\n    # mlflow.log_figure(...)  # <-- Complétez ceci\n    # plt.close()\n    \n    print(\"Exercice terminé !\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.3 Défi : Trouver et charger le meilleur modèle\n\nUtilisez le Client MLflow pour :\n1. Trouver le run avec la meilleure accuracy\n2. Charger le modèle de ce run\n3. Faire des prédictions sur de nouvelles données"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DÉFI : Complétez ce code\n\nfrom mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\nexperiment = client.get_experiment_by_name(experiment_name)\n\n# TODO : Rechercher le meilleur run par accuracy\n# best_runs = client.search_runs(\n#     experiment_ids=[...],\n#     order_by=[...],\n#     max_results=1\n# )\n\n# if best_runs:\n#     best_run = best_runs[0]\n#     print(f\"Meilleur run : {best_run.info.run_name}\")\n#     print(f\"Accuracy : {best_run.data.metrics.get('accuracy', 0):.4f}\")\n    \n#     # TODO : Charger le modèle\n#     # model_uri = f\"runs:/{best_run.info.run_id}/model\"\n#     # loaded_model = mlflow.sklearn.load_model(model_uri)\n    \n#     # TODO : Faire des prédictions\n#     # predictions = loaded_model.predict(X_test[:5])\n#     # print(f\"Exemples de prédictions : {predictions}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# PARTIE 4 : Serving & Inférence\n\nVous savez déjà servir des modèles avec joblib + FastAPI. Voyons comment MLflow simplifie cela."
  },
  {
   "cell_type": "markdown",
   "source": "## 4.1 Enregistrer un modèle\n\nAvant de servir, enregistrons notre meilleur modèle dans le Model Registry.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# D'abord, entraînons un modèle qu'on veut enregistrer\nwith mlflow.start_run(run_name=\"modele-pour-registry\") as run:\n    \n    mlflow.log_params({\"n_estimators\": 200, \"max_depth\": 12})\n    \n    model = RandomForestClassifier(n_estimators=200, max_depth=12, random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    mlflow.log_metrics({\n        \"accuracy\": accuracy_score(y_test, y_pred),\n        \"f1\": f1_score(y_test, y_pred)\n    })\n    \n    # Logger le modèle\n    mlflow.sklearn.log_model(model, name=\"model\")\n    \n    # Sauvegarder run_id pour l'enregistrement\n    run_id = run.info.run_id\n    print(f\"Modèle loggé. Run ID : {run_id}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Enregistrer le modèle dans le Model Registry\nmodel_name = \"churn-predictor\"\nmodel_uri = f\"runs:/{run_id}/model\"\n\n# Enregistrer !\nmodel_version = mlflow.register_model(model_uri, model_name)\n\nprint(f\"Modèle enregistré : {model_name}\")\nprint(f\"Version : {model_version.version}\")\nprint(f\"\\nVoir dans l'UI : http://localhost:5000/#/models/{model_name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.2 Charger un modèle depuis le Registry\n\nAu lieu de `joblib.load('model.pkl')`, vous pouvez charger par nom et version.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Charger le modèle depuis le registry - comparez avec ce que vous connaissez !\n\n# ANCIENNE MÉTHODE (ce que vous connaissez déjà) :\n# model = joblib.load('model.pkl')  # Où est-il ? Quelle version ?\n\n# MÉTHODE MLFLOW :\n# Charger la dernière version\nloaded_model = mlflow.sklearn.load_model(f\"models:/{model_name}/latest\")\n\n# Ou charger une version spécifique\n# loaded_model = mlflow.sklearn.load_model(f\"models:/{model_name}/1\")\n\n# Faire des prédictions - identique à avant !\nsample_predictions = loaded_model.predict(X_test[:5])\nprint(f\"Exemples de prédictions : {sample_predictions}\")\nprint(f\"\\nModèle chargé depuis le registry : {model_name}/latest\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.3 Servir le modèle comme API REST\n\nVous savez comment servir des modèles avec FastAPI :\n\n```python\n# ANCIENNE MÉTHODE - FastAPI + joblib\nfrom fastapi import FastAPI\nimport joblib\n\napp = FastAPI()\nmodel = joblib.load('model.pkl')\n\n@app.post(\"/predict\")\ndef predict(data: dict):\n    X = pd.DataFrame([data])\n    return {\"prediction\": model.predict(X).tolist()}\n```\n\n**MLflow fait ça en une commande :**\n\n```bash\nmlflow models serve -m models:/churn-predictor/latest -p 5001 --no-conda\n```\n\nC'est tout. Pas besoin de code FastAPI.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Si vous exécutez la commande serve ci-dessus, vous pouvez l'appeler comme ceci :\n# (C'est le même pattern que vous connaissez avec FastAPI !)\n\nimport requests\n\n# Données exemple pour la prédiction\nsample_data = X_test.head(3).to_dict(orient='split')\n\n# Décommentez quand le serveur tourne :\n# response = requests.post(\n#     \"http://localhost:5001/invocations\",\n#     json={\"dataframe_split\": sample_data}\n# )\n# print(response.json())\n\nprint(\"Pour tester le serving :\")\nprint(\"1. Ouvrez un terminal\")\nprint(\"2. Exécutez : mlflow models serve -m models:/churn-predictor/latest -p 5001 --no-conda\")\nprint(\"3. Décommentez le code ci-dessus et exécutez cette cellule\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.4 Inférence Batch (Charger depuis le Registry)\n\nPour des prédictions batch dans un pipeline, chargez et prédisez simplement :",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Inférence batch - c'est ce que les orchestrateurs vont faire !\n\n# Charger le modèle depuis le registry\nmodel = mlflow.sklearn.load_model(f\"models:/{model_name}/latest\")\n\n# Charger de nouvelles données (simuler un batch quotidien)\nnew_customers = df[FEATURE_COLS].head(100)\n\n# Prédire\npredictions = model.predict(new_customers)\nprobabilities = model.predict_proba(new_customers)[:, 1]\n\n# Créer les résultats\nresults = pd.DataFrame({\n    'customer_id': df['customer_id'].head(100),\n    'churn_probability': probabilities,\n    'churn_predicted': predictions\n})\n\nprint(f\"Prédictions batch pour {len(results)} clients :\")\nprint(results.head(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.5 Comparaison : Ce que vous connaissez vs MLflow\n\n| Tâche | Manuel (joblib + FastAPI) | MLflow |\n|-------|---------------------------|--------|\n| Sauvegarder modèle | `joblib.dump(model, 'model_v2.pkl')` | `mlflow.sklearn.log_model(model, name=\"model\")` |\n| Charger modèle | `joblib.load('model_v2.pkl')` | `mlflow.sklearn.load_model(\"models:/name/1\")` |\n| Versioning | Noms de fichiers manuels (`model_v1.pkl`, `model_v2.pkl`) | Versions automatiques (1, 2, 3...) |\n| Servir en API | Écrire du code FastAPI, lancer uvicorn | `mlflow models serve -m models:/name/1` |\n| Tracker quel modèle | Espérer se souvenir / vérifier les dates de fichiers | Registry montre version, métriques, qui l'a entraîné |\n| Rollback | Trouver l'ancien fichier, espérer qu'il marche | `mlflow.sklearn.load_model(\"models:/name/1\")` |\n\n**MLflow ne remplace pas vos compétences** - il les enrichit avec le versioning, le tracking et l'automatisation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Résumé\n\n## Ce que vous avez appris\n\n| Concept | Code |\n|---------|------|\n| Se connecter à MLflow | `mlflow.set_tracking_uri(\"http://localhost:5000\")` |\n| Créer une expérience | `mlflow.set_experiment(\"name\")` |\n| Démarrer un run | `with mlflow.start_run():` |\n| Logger des paramètres | `mlflow.log_params({\"key\": value})` |\n| Logger des métriques | `mlflow.log_metrics({\"accuracy\": 0.95})` |\n| Logger un modèle | `mlflow.sklearn.log_model(model, name=\"model\")` |\n| Logger une figure | `mlflow.log_figure(fig, \"plot.png\")` |\n| Logger un artefact | `mlflow.log_artifact(\"file.pkl\")` |\n| Définir des tags | `mlflow.set_tag(\"author\", \"me\")` |\n| Rechercher des runs | `client.search_runs(...)` |\n| Enregistrer un modèle | `mlflow.register_model(uri, \"name\")` |\n| Charger depuis le registry | `mlflow.sklearn.load_model(\"models:/name/latest\")` |\n| Servir un modèle | `mlflow models serve -m models:/name/1 -p 5001` |\n\n## Le Pipeline ML Complet\n\n```\n┌──────────────┐    ┌──────────────┐    ┌──────────────┐    ┌──────────────┐\n│  DÉVELOPPER  │ →  │  ENTRAÎNER   │ →  │  ENREGISTRER │ →  │    SERVIR    │\n├──────────────┤    ├──────────────┤    ├──────────────┤    ├──────────────┤\n│  Notebooks   │    │ Orchestrateur│    │    Model     │    │  API REST    │\n│ Expériences  │    │  + Tracking  │    │   Registry   │    │   ou Batch   │\n└──────────────┘    └──────────────┘    └──────────────┘    └──────────────┘\n    Partie 1-3         Partie 4           Partie 4           Partie 4\n```\n\n## Prochaines étapes\n\n**Notebook de référence** : Voir `02_mlflow_organized.ipynb` pour la version complète, prête pour la production.\n\n**Orchestrateurs** : Maintenant que vous comprenez MLflow, automatisons ce pipeline !\n- **Prefect** : Voir `pipelines/Prefect_ML_Pipeline.py` - entraîne ET fait de l'inférence\n- **Dagster** : Voir `pipelines/Dagster_ML_Pipeline.py` - approche centrée sur les assets\n- **Airflow** : Voir `pipelines/Airflow_ML_Pipeline.py` - standard de l'industrie\n\nLes orchestrateurs vont :\n1. Planifier des runs d'entraînement quotidiens\n2. Logger tout dans MLflow automatiquement\n3. Enregistrer les nouvelles versions de modèles\n4. Exécuter l'inférence batch et sauvegarder les prédictions\n\n---",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}