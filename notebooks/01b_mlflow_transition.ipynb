{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Chaos to MLflow: A Guided Journey\n",
    "\n",
    "## Workshop Objective\n",
    "\n",
    "You just experienced the **messy notebook** (`01_messy_notebook.ipynb`). You probably noticed:\n",
    "\n",
    "- \"Wait, which model was the best again?\" \n",
    "- Metrics scattered across cells, hard to compare\n",
    "- No way to reproduce exact results\n",
    "- Manual tracking in markdown tables (that you forgot to update)\n",
    "\n",
    "**In this notebook, we'll fix all of that with MLflow.**\n",
    "\n",
    "---\n",
    "\n",
    "## How This Notebook Works\n",
    "\n",
    "We'll progressively increase the difficulty:\n",
    "\n",
    "| Part | Style | You will... |\n",
    "|------|-------|-------------|\n",
    "| **Part 1** | Fully Guided | Run code, observe, learn concepts |\n",
    "| **Part 2** | Progressive Reveal | Predict what's needed, then see solution |\n",
    "| **Part 3** | Fill-in-the-Blanks | Complete the code yourself |\n",
    "\n",
    "**Reference**: See `02_mlflow_organized.ipynb` for the complete solution.\n",
    "\n",
    "**Cheatsheet**: See `../docs/mlflow_cheatsheet.md` for quick reference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure:\n",
    "1. MLflow server is running: `docker-compose up -d` (from project root)\n",
    "2. Check http://localhost:5000 is accessible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load the Messy Notebook Data\n",
    "\n",
    "First, let's reproduce the same data preparation from the messy notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports (same as messy notebook)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Imports done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data (same as messy notebook)\n",
    "df = pd.read_csv('../data/customer_data.csv')\n",
    "\n",
    "# Feature engineering\n",
    "df['recency_frequency_ratio'] = df['recency_days'] / (df['frequency'] + 1)\n",
    "df['monetary_per_order'] = df['monetary_value'] / (df['total_orders'] + 1)\n",
    "df['order_frequency'] = df['total_orders'] / (df['days_since_signup'] + 1)\n",
    "df['support_per_order'] = df['support_tickets'] / (df['total_orders'] + 1)\n",
    "df['r_score'] = pd.qcut(df['recency_days'], q=5, labels=[5, 4, 3, 2, 1]).astype(int)\n",
    "df['f_score'] = pd.qcut(df['frequency'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5]).astype(int)\n",
    "df['m_score'] = pd.qcut(df['monetary_value'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5]).astype(int)\n",
    "df['rfm_score'] = df['r_score'] + df['f_score'] + df['m_score']\n",
    "\n",
    "# Define features\n",
    "FEATURE_COLS = [\n",
    "    'recency_days', 'frequency', 'monetary_value', 'avg_order_value',\n",
    "    'days_since_signup', 'total_orders', 'support_tickets', 'age',\n",
    "    'recency_frequency_ratio', 'monetary_per_order', 'order_frequency',\n",
    "    'support_per_order', 'rfm_score'\n",
    "]\n",
    "\n",
    "X = df[FEATURE_COLS]\n",
    "y = df['churned']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Data ready: {len(X_train)} train, {len(X_test)} test samples\")\n",
    "print(f\"Features: {len(FEATURE_COLS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# PART 1: Fully Guided\n\n## 1.1 What is MLflow?\n\nMLflow is an open-source platform for managing the **full ML lifecycle**. It has 4 core components:\n\n### 1. **Tracking** (our main focus today)\nLog and query experiments: parameters, metrics, artifacts, and code versions.\n- *\"What hyperparameters did I use for my best model?\"*\n- *\"Compare accuracy across 50 runs\"*\n\n### 2. **Models**\nPackage ML models in a standard format for deployment to various platforms (Docker, Kubernetes, cloud services, etc.).\n- *\"Deploy this sklearn model as a REST API\"*\n- *\"Convert my model for batch inference\"*\n\n### 3. **Model Registry** (we'll use this too)\nCentralized model store with versioning, stage transitions (Staging → Production), and annotations.\n- *\"Promote model v3 to production\"*\n- *\"Who approved this model version?\"*\n\n### 4. **Projects**\nPackage code and dependencies for reproducible runs on any platform.\n- *\"Run this experiment with the exact same environment\"*\n- *\"Share my pipeline with the team\"*\n\n---\n\n### MLflow for LLMs (Bonus knowledge)\n\nMLflow now supports **LLM/GenAI applications**:\n- **Tracing**: Debug and monitor LLM calls, RAG pipelines, and multi-step agents\n- **Evaluation**: Benchmark LLM outputs with built-in metrics\n- **Deployment**: Serve LLM models with the same model-serving infrastructure\n\n*We won't cover LLM tracking today, but know that MLflow works beyond traditional ML!*\n\n---\n\n**Today we focus on Tracking and Registry** - the foundation for experiment management."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Connect to MLflow\n",
    "\n",
    "First, we import MLflow and tell it where our tracking server is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: Import MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the MLflow server (running in Docker)\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# Verify connection\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:5000/health\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"MLflow server is running!\")\n",
    "    else:\n",
    "        print(f\"Server responded with: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Cannot connect to MLflow server: {e}\")\n",
    "    print(\"Run 'docker-compose up -d' from the project root!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Create an Experiment\n",
    "\n",
    "An **Experiment** groups related runs together. Think of it as a project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get an experiment\n",
    "experiment_name = \"workshop-churn-learning\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"Using experiment: {experiment_name}\")\n",
    "print(f\"View it at: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Your First MLflow Run\n",
    "\n",
    "A **Run** is a single execution of your training code. Inside a run, you can log:\n",
    "\n",
    "- **Parameters**: Inputs (hyperparameters, config)\n",
    "- **Metrics**: Outputs (accuracy, loss, F1)\n",
    "- **Artifacts**: Files (models, plots, data)\n",
    "- **Tags**: Metadata (author, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a simple model and LOG EVERYTHING\n",
    "\n",
    "# Start a run\n",
    "with mlflow.start_run(run_name=\"my-first-mlflow-run\"):\n",
    "    \n",
    "    # 1. LOG PARAMETERS (inputs to your model)\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    \n",
    "    # 2. Train the model (same code as before)\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 4. LOG METRICS (outputs/results)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nRun logged to MLflow!\")\n",
    "    print(\"Go to http://localhost:5000 to see it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go check the MLflow UI!\n",
    "\n",
    "1. Open http://localhost:5000\n",
    "2. Click on \"workshop-churn-learning\" experiment\n",
    "3. Click on \"my-first-mlflow-run\"\n",
    "4. See your parameters and metrics!\n",
    "\n",
    "**This is already better than the messy notebook** - your results are saved and organized!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Log Multiple Metrics at Once\n",
    "\n",
    "Instead of calling `log_metric` multiple times, you can log a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More efficient: log multiple metrics at once\n",
    "with mlflow.start_run(run_name=\"multiple-metrics-example\"):\n",
    "    \n",
    "    # Log params as dict\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"RandomForest\",\n",
    "        \"n_estimators\": 150,\n",
    "        \"max_depth\": 12\n",
    "    })\n",
    "    \n",
    "    # Train\n",
    "    model = RandomForestClassifier(n_estimators=150, max_depth=12, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Log metrics as dict\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred)\n",
    "    })\n",
    "    \n",
    "    print(\"All metrics logged!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2: Progressive Reveal\n",
    "\n",
    "Now you understand the basics. Let's add more MLflow features.\n",
    "\n",
    "**Format**: I'll show you a problem, you think about what's needed, then reveal the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Problem: Saving the Model\n",
    "\n",
    "In the messy notebook, we had:\n",
    "```python\n",
    "# save model? idk where\n",
    "# import pickle\n",
    "# with open('model.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_rf, f)\n",
    "```\n",
    "\n",
    "**Question**: How can we save the model WITH the run, so it's always linked to its metrics?\n",
    "\n",
    "*Think about it... then run the next cell for the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Use mlflow.sklearn.log_model()\n",
    "\n",
    "with mlflow.start_run(run_name=\"with-model-artifact\"):\n",
    "    \n",
    "    mlflow.log_params({\"n_estimators\": 100, \"max_depth\": 10})\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred)\n",
    "    })\n",
    "    \n",
    "    # LOG THE MODEL - it's now saved with this run!\n",
    "    mlflow.sklearn.log_model(model, name=\"model\")\n",
    "    \n",
    "    print(\"Model saved as artifact!\")\n",
    "    print(\"Check the 'Artifacts' tab in the MLflow UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Problem: Saving Plots\n",
    "\n",
    "In the messy notebook, we created confusion matrices and feature importance plots.\n",
    "But they just displayed in the notebook - not saved anywhere!\n",
    "\n",
    "**Question**: How can we save a matplotlib figure as an artifact?\n",
    "\n",
    "*Think... then reveal.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Use mlflow.log_figure()\n",
    "\n",
    "with mlflow.start_run(run_name=\"with-plots\"):\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mlflow.log_metrics({\"accuracy\": accuracy_score(y_test, y_pred)})\n",
    "    \n",
    "    # Create confusion matrix plot\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    \n",
    "    # LOG THE FIGURE - no local file created!\n",
    "    mlflow.log_figure(fig, \"confusion_matrix.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Plot saved as artifact!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Problem: The Scaler Issue\n",
    "\n",
    "For Logistic Regression, we need to scale the features:\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "```\n",
    "\n",
    "**Problem**: At inference time, we need the SAME scaler. If we don't save it, we can't make correct predictions later!\n",
    "\n",
    "**Question**: How do we save the scaler with the model?\n",
    "\n",
    "*Think... then reveal.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Save the scaler as an artifact\n",
    "import joblib\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "with mlflow.start_run(run_name=\"logistic-with-scaler\"):\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Save scaler as artifact (using tempfile to avoid leaving files)\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        scaler_path = os.path.join(tmpdir, \"scaler.pkl\")\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        mlflow.log_artifact(scaler_path, artifact_path=\"preprocessing\")\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    mlflow.log_metrics({\"accuracy\": accuracy_score(y_test, y_pred)})\n",
    "    mlflow.sklearn.log_model(model, name=\"model\")\n",
    "    \n",
    "    # Tag to remember this model needs scaling\n",
    "    mlflow.set_tag(\"requires_scaling\", \"true\")\n",
    "    \n",
    "    print(\"Model + scaler saved!\")\n",
    "    print(\"Check artifacts: preprocessing/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Problem: Comparing Runs\n",
    "\n",
    "In the messy notebook:\n",
    "```python\n",
    "# ok I give up trying to track all these experiments\n",
    "# which one was the best? I think it was the grid search one?\n",
    "```\n",
    "\n",
    "**Question**: How do we programmatically find the best run?\n",
    "\n",
    "*Think... then reveal.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Use MLflow Client to search runs\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Search runs, ordered by accuracy (descending)\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"metrics.accuracy DESC\"],\n",
    "    max_results=5\n",
    ")\n",
    "\n",
    "print(\"Top 5 runs by accuracy:\")\n",
    "print(\"=\" * 60)\n",
    "for run in runs:\n",
    "    acc = run.data.metrics.get('accuracy', 0)\n",
    "    f1 = run.data.metrics.get('f1', 0)\n",
    "    print(f\"{run.info.run_name}: accuracy={acc:.4f}, f1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: Fill in the Blanks\n",
    "\n",
    "Now it's your turn! Complete the code in the following cells.\n",
    "\n",
    "**Hint**: Refer to the cheatsheet at `../docs/mlflow_cheatsheet.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Exercise: Log a Gradient Boosting Model\n",
    "\n",
    "Complete the code to train and log a Gradient Boosting model with:\n",
    "- Parameters: n_estimators, learning_rate, max_depth\n",
    "- Metrics: accuracy, precision, recall, f1\n",
    "- Artifact: the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Complete this code\n",
    "\n",
    "with mlflow.start_run(run_name=\"exercise-gradient-boosting\"):\n",
    "    \n",
    "    # TODO: Log these parameters\n",
    "    n_estimators = 100\n",
    "    learning_rate = 0.1\n",
    "    max_depth = 5\n",
    "    \n",
    "    # mlflow.log_params({...})  # <-- Complete this\n",
    "    \n",
    "    # Train model\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # TODO: Calculate and log metrics\n",
    "    # mlflow.log_metrics({...})  # <-- Complete this\n",
    "    \n",
    "    # TODO: Log the model\n",
    "    # mlflow.sklearn.log_model(...)  # <-- Complete this\n",
    "    \n",
    "    print(\"Exercise completed! Check MLflow UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "with mlflow.start_run(run_name=\"exercise-gradient-boosting\"):\n",
    "    \n",
    "    n_estimators = 100\n",
    "    learning_rate = 0.1\n",
    "    max_depth = 5\n",
    "    \n",
    "    mlflow.log_params({\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"max_depth\": max_depth\n",
    "    })\n",
    "    \n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred)\n",
    "    })\n",
    "    \n",
    "    mlflow.sklearn.log_model(model, name=\"model\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Exercise: Add Tags and a Confusion Matrix\n",
    "\n",
    "Enhance your run with:\n",
    "- Tags: author (your name), model_type\n",
    "- A confusion matrix plot as artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Complete this code\n",
    "\n",
    "with mlflow.start_run(run_name=\"exercise-with-tags-and-plot\"):\n",
    "    \n",
    "    # TODO: Add tags\n",
    "    # mlflow.set_tag(\"author\", \"...\")  # <-- Your name\n",
    "    # mlflow.set_tag(\"model_type\", \"...\")  # <-- Model type\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # TODO: Create and log confusion matrix\n",
    "    # cm = confusion_matrix(y_test, y_pred)\n",
    "    # fig, ax = plt.subplots(...)\n",
    "    # sns.heatmap(...)\n",
    "    # mlflow.log_figure(...)  # <-- Complete this\n",
    "    # plt.close()\n",
    "    \n",
    "    print(\"Exercise completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Challenge: Find and Load the Best Model\n",
    "\n",
    "Use the MLflow Client to:\n",
    "1. Find the run with the highest accuracy\n",
    "2. Load the model from that run\n",
    "3. Make predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHALLENGE: Complete this code\n",
    "\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# TODO: Search for the best run by accuracy\n",
    "# best_runs = client.search_runs(\n",
    "#     experiment_ids=[...],\n",
    "#     order_by=[...],\n",
    "#     max_results=1\n",
    "# )\n",
    "\n",
    "# if best_runs:\n",
    "#     best_run = best_runs[0]\n",
    "#     print(f\"Best run: {best_run.info.run_name}\")\n",
    "#     print(f\"Accuracy: {best_run.data.metrics.get('accuracy', 0):.4f}\")\n",
    "    \n",
    "#     # TODO: Load the model\n",
    "#     # model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "#     # loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "    \n",
    "#     # TODO: Make predictions\n",
    "#     # predictions = loaded_model.predict(X_test[:5])\n",
    "#     # print(f\"Sample predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# PART 4: Serving & Inference\n\nYou already know how to serve models with joblib + FastAPI. Let's see how MLflow simplifies this."
  },
  {
   "cell_type": "markdown",
   "source": "## 4.1 Register a Model\n\nBefore serving, let's register our best model in the Model Registry.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# First, let's train a model we want to register\nwith mlflow.start_run(run_name=\"model-for-registry\") as run:\n    \n    mlflow.log_params({\"n_estimators\": 200, \"max_depth\": 12})\n    \n    model = RandomForestClassifier(n_estimators=200, max_depth=12, random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    mlflow.log_metrics({\n        \"accuracy\": accuracy_score(y_test, y_pred),\n        \"f1\": f1_score(y_test, y_pred)\n    })\n    \n    # Log the model\n    mlflow.sklearn.log_model(model, name=\"model\")\n    \n    # Save run_id for registration\n    run_id = run.info.run_id\n    print(f\"Model logged. Run ID: {run_id}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Register the model in the Model Registry\nmodel_name = \"churn-predictor\"\nmodel_uri = f\"runs:/{run_id}/model\"\n\n# Register!\nmodel_version = mlflow.register_model(model_uri, model_name)\n\nprint(f\"Model registered: {model_name}\")\nprint(f\"Version: {model_version.version}\")\nprint(f\"\\nView in UI: http://localhost:5000/#/models/{model_name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.2 Load Model from Registry\n\nInstead of `joblib.load('model.pkl')`, you can load by name and version.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load model from registry - compare with what you know!\n\n# OLD WAY (what you already know):\n# model = joblib.load('model.pkl')  # Where is it? Which version?\n\n# MLFLOW WAY:\n# Load latest version\nloaded_model = mlflow.sklearn.load_model(f\"models:/{model_name}/latest\")\n\n# Or load specific version\n# loaded_model = mlflow.sklearn.load_model(f\"models:/{model_name}/1\")\n\n# Make predictions - same as before!\nsample_predictions = loaded_model.predict(X_test[:5])\nprint(f\"Sample predictions: {sample_predictions}\")\nprint(f\"\\nModel loaded from registry: {model_name}/latest\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.3 Serve Model as REST API\n\nYou know how to serve models with FastAPI:\n\n```python\n# OLD WAY - FastAPI + joblib\nfrom fastapi import FastAPI\nimport joblib\n\napp = FastAPI()\nmodel = joblib.load('model.pkl')\n\n@app.post(\"/predict\")\ndef predict(data: dict):\n    X = pd.DataFrame([data])\n    return {\"prediction\": model.predict(X).tolist()}\n```\n\n**MLflow does this in one command:**\n\n```bash\nmlflow models serve -m models:/churn-predictor/latest -p 5001 --no-conda\n```\n\nThat's it. No FastAPI code needed.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# If you run the serve command above, you can call it like this:\n# (This is the same pattern you know from FastAPI!)\n\nimport requests\n\n# Sample data for prediction\nsample_data = X_test.head(3).to_dict(orient='split')\n\n# Uncomment when server is running:\n# response = requests.post(\n#     \"http://localhost:5001/invocations\",\n#     json={\"dataframe_split\": sample_data}\n# )\n# print(response.json())\n\nprint(\"To test serving:\")\nprint(\"1. Open a terminal\")\nprint(\"2. Run: mlflow models serve -m models:/churn-predictor/latest -p 5001 --no-conda\")\nprint(\"3. Uncomment the code above and run this cell\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.4 Batch Inference (Load from Registry)\n\nFor batch predictions in a pipeline, just load and predict:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Batch inference - this is what orchestrators will do!\n\n# Load model from registry\nmodel = mlflow.sklearn.load_model(f\"models:/{model_name}/latest\")\n\n# Load new data (simulate daily batch)\nnew_customers = df[FEATURE_COLS].head(100)\n\n# Predict\npredictions = model.predict(new_customers)\nprobabilities = model.predict_proba(new_customers)[:, 1]\n\n# Create results\nresults = pd.DataFrame({\n    'customer_id': df['customer_id'].head(100),\n    'churn_probability': probabilities,\n    'churn_predicted': predictions\n})\n\nprint(f\"Batch predictions for {len(results)} customers:\")\nprint(results.head(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.5 Comparison: What You Know vs MLflow\n\n| Task | Manual (joblib + FastAPI) | MLflow |\n|------|---------------------------|--------|\n| Save model | `joblib.dump(model, 'model_v2.pkl')` | `mlflow.sklearn.log_model(model, name=\"model\")` |\n| Load model | `joblib.load('model_v2.pkl')` | `mlflow.sklearn.load_model(\"models:/name/1\")` |\n| Versioning | Manual file names (`model_v1.pkl`, `model_v2.pkl`) | Automatic versions (1, 2, 3...) |\n| Serve API | Write FastAPI code, run uvicorn | `mlflow models serve -m models:/name/1` |\n| Track which model | Hope you remember / check file dates | Registry shows version, metrics, who trained it |\n| Rollback | Find old file, hope it works | `mlflow.sklearn.load_model(\"models:/name/1\")` |\n\n**MLflow doesn't replace your skills** - it builds on them with versioning, tracking, and automation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Summary\n\n## What You Learned\n\n| Concept | Code |\n|---------|------|\n| Connect to MLflow | `mlflow.set_tracking_uri(\"http://localhost:5000\")` |\n| Create experiment | `mlflow.set_experiment(\"name\")` |\n| Start a run | `with mlflow.start_run():` |\n| Log parameters | `mlflow.log_params({\"key\": value})` |\n| Log metrics | `mlflow.log_metrics({\"accuracy\": 0.95})` |\n| Log model | `mlflow.sklearn.log_model(model, name=\"model\")` |\n| Log figure | `mlflow.log_figure(fig, \"plot.png\")` |\n| Log artifact | `mlflow.log_artifact(\"file.pkl\")` |\n| Set tags | `mlflow.set_tag(\"author\", \"me\")` |\n| Search runs | `client.search_runs(...)` |\n| Register model | `mlflow.register_model(uri, \"name\")` |\n| Load from registry | `mlflow.sklearn.load_model(\"models:/name/latest\")` |\n| Serve model | `mlflow models serve -m models:/name/1 -p 5001` |\n\n## The Full ML Pipeline\n\n```\n┌──────────────┐    ┌──────────────┐    ┌──────────────┐    ┌──────────────┐\n│   DEVELOP    │ →  │    TRAIN     │ →  │   REGISTER   │ →  │    SERVE     │\n├──────────────┤    ├──────────────┤    ├──────────────┤    ├──────────────┤\n│  Notebooks   │    │  Orchestrator│    │   Model      │    │  REST API    │\n│  Experiments │    │  + Tracking  │    │   Registry   │    │  or Batch    │\n└──────────────┘    └──────────────┘    └──────────────┘    └──────────────┘\n     Part 1-3            Part 4              Part 4            Part 4\n```\n\n## Next Steps\n\n**Reference notebook**: See `02_mlflow_organized.ipynb` for the complete, production-ready version.\n\n**Orchestrators**: Now that you understand MLflow, let's automate this pipeline!\n- **Prefect**: See `pipelines/Prefect_ML_Pipeline.py` - trains AND runs inference\n- **Dagster**: See `pipelines/Dagster_ML_Pipeline.py` - asset-centric approach\n- **Airflow**: See `pipelines/Airflow_ML_Pipeline.py` - industry standard\n\nThe orchestrators will:\n1. Schedule daily training runs\n2. Log everything to MLflow automatically\n3. Register new model versions\n4. Run batch inference and save predictions\n\n---",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}